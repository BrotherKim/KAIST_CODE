{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization__Sentence_Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrotherKim/Colab/blob/main/SEP531/Tokenization__Sentence_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FI1-ppwg657"
      },
      "source": [
        "# [SEP 531] 정보검색 및 자연어처리 실습: Preprocessing (Tokenization & Sentence Segmenation)\n",
        "\n",
        "Primary TA: 이영준\n",
        "\n",
        "TA's E-mail: passing2961@gmail.com\n",
        "\n",
        "본 실습은 말뭉치를 특정 unit 단위로 쪼개는 tokenization 과정에서 사용되는 모델에는 어떤 것이 있는지를 살펴보며, 한국어를 직접 다뤄보는 것을 목표로 하고 있습니다. Tokenization 은 자연어처리 태스크를 수행하기 위해 필수적으로 거쳐야하는 과정이며, 어떤 tokenization 모델을 사용하냐에 따라 성능 차이가 나타나는 것을 알 수 있습니다. 최근에는, 코퍼스를 subword 단위로 분절화하는 Byte Pair Encoding (BPE) 방법이 사용되고 있으며, 그 변형으로 BERT 에서 사용하는 Word Piece Model (WPM), Google 의 SentencePiece Model, GPT2&RoBERTa 에서 사용되는 Byte-level BPE 등이 있습니다. 한국어의 경우에는 교착어라는 특성이 있으므로, 영어와 다르게 형태소 품사를 통해 분석된 결과 문장에 대해 BPE 를 적용하는, Morepheme-aware BPE 방법을 사용하고 있습니다. \n",
        "\n",
        "\n",
        "## Contents\n",
        "\n",
        "- What is the Tokenization?\n",
        "- Variants of Tokenization Model:\n",
        "  - Jamo\n",
        "  - Character\n",
        "  - Word\n",
        "  - Morepheme\n",
        "  - Subword\n",
        "  - Morepheme-aware Subword\n",
        "- Sentence Splitter (문장 분리기)\n",
        "\n",
        "[link1]: https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "[link2]: https://github.com/kakaobrain/pororo\n",
        "[link3]: https://github.com/zaemyung/sentsplit\n",
        "[link4]: https://arxiv.org/abs/2010.02534\n",
        "[link5]: https://huggingface.co/docs/tokenizers/python/latest/index.html\n",
        "\n",
        "## References\n",
        "\n",
        "- What is Tokenization in NLP? Here’s All You Need To Know ([link][link1])\n",
        "- `Pororo` library ([link][link2])\n",
        "- 'sentsplit` library ([link][link3])\n",
        "- An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks ([arXiv-link][link4])\n",
        "- `huggingface`'s tokenizer ([link][link5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1eELFFzAAHa"
      },
      "source": [
        "## What is the Tokenization?\n",
        "\n",
        "Tokenization 은 NLP 에서 필수적인 과정으로써, 자연어 문장 혹은 문서를 기계 (인공지능 모델, 컴퓨터 등)가 이해할 수 있도록 해주는 역할을 합니다. 즉, 어떠한 자연어 문장이 주어져도 모델이 이해할 수 있도록 자연어 문장을 분절 (break down) 하는 과정을 의미합니다. 이러한 Tokenization 은 전통적인 인공지능 기법에서 부터 딥러닝 모델들에서까지 사용되고 있습니다.\n",
        "\n",
        "#### **Definition of Tokenization**\n",
        "Tokenization 은 자연어로 이루어진 문장을 인공지능 모델이 이해할 수 있도록 작은 단위로 나누는 과정을 의미하며, 여기서 작은 단위를 소위 **`token`** 이라고 부릅니다. `Token` 의 형태는 다양하며 가장 대표적으로 사용되는 단위로는 Word, Character, Subword 등이 있습니다. 한국어의 경우에는 교착어라는 특성상 Morepheme, Constant & Vowel (자모), Syllable (어절) 등이 추가로 사용되고 있습니다. 아래의 예제를 통해 어떻게 나눠지는지 살펴보도록 하겠습니다.\n",
        "\n",
        "- 입력 문장: 나랑 쇼핑하자\n",
        "\n",
        "|Tokenization|Tokenized Sequence|\n",
        "|------|---|\n",
        "|Constant and Vowel (자모 단위)|ㄴ/ㅏ/ㄹ/ㅏ/ㅇ/*/ㅅ/ㅛ/ㅍ/ㅣ/ㅇ/ㅎ/ㅏ/ㅈ/ㅏ/.|\n",
        "|Syllable (음절 단위)|나/랑/*/쇼/핑/하/자/.|\n",
        "|Word (어절, 단어 단위)|나랑/쇼핑하자/.|\n",
        "|Morpheme (형태소 단위)| 나/랑/*/쇼핑/하/자/.|\n",
        "|Subword (서브워드 단위)|_나랑/_쇼/핑하/자/.|\n",
        "|Morpheme-aware Subword (형태소 지향 서브워드 단위)|_나/_랑/*/_쇼/핑/_하/_자/_.|\n",
        "\n",
        "위에 기술된 예제처럼, 어떤 Tokenization 방법을 쓰냐에 따라서 생성되는 토큰의 형태가 매우 다양한 것을 확인할 수 있습니다. 토큰의 형태가 다양함에 따라, 모델의 학습에 사용되는 vocab 의 형태도 달라지게 되며 이는 직접적으로 성능에 영향을 주기도 합니다. 최근, Tokenization 방법들을 다양하게 바꿔가면서 한국어 관련 NLU task 에 성능 변화에 어떤 영향을 끼치는지 실험적으로 분석한 논문도 공개가 되어있습니다. 해당 논문에서는 KorQuAD 태스크에서는 Subword 단위가, KorNLI, KorSTS, NSMC, PAWS-X 태스크에서는 Morpheme-aware Subword 단위가 가장 좋은 성능을 달성한 것으로 나타나고 있습니다.\n",
        "\n",
        "#### **What about the Korean?**\n",
        "\n",
        "| 원형 | 피동 | 높임 | 과거 | 추측 | 전달 |   | 결과 |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| 잡 |  |  |  |  |  | +다 | 잡다 |\n",
        "| 잡 | +히 |  |  |  |  | +다 | 잡히다 |\n",
        "| 잡 | +히 | +시 |  |  |  | +다 | 잡히시다 |\n",
        "| 잡 | +히 | +시 | +었 |  |  | +다 | 잡히셨다 |\n",
        "| 잡 |  |  | +았(었) |  |  | +다 | 잡았다 |\n",
        "| 잡 |  |  |  | +겠 |  | +다 | 잡겠다 |\n",
        "| 잡 |  |  |  |  | +더라 |  | 잡더라 |\n",
        "| 잡 | +히 |  | +었 |  |  | +다 | 잡혔다 |\n",
        "\n",
        "- 한국어는 교착어\n",
        "  - 단어에 조사가 붙어 의미와 문법적 기능이 부여\n",
        "    - e.g. 그가, 그에게, 그를, 그와, 그는\n",
        "  - 형태소 (morpheme): 뜻을 가진 가장 작은 말의 단위\n",
        "\n",
        "- 한국어는 띄어쓰기가 잘 지켜지지 않음\n",
        "  - \"띄어쓰기를전혀하지않아도글이무슨의미인지이해할수있습니다.\"\n",
        "\n",
        "\n",
        "[konlpy]: https://konlpy.org/en/latest/\n",
        "[khaiii]: https://github.com/kakao/khaiii\n",
        "[ETRI Open API]: http://aiopen.etri.re.kr/service_api.php\n",
        "[Pynori]: https://github.com/gritmind/python-nori\n",
        "[Mecab]: https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/\n",
        "[Pororo]: https://github.com/kakaobrain/pororo\n",
        "\n",
        "- (공개) 형태소 분석기\n",
        "  - [konlpy]\n",
        "  - [khaiii] \n",
        "  - [ETRI Open API]\n",
        "  - [Pynori]\n",
        "  - [Mecab]\n",
        "  - [Pororo]\n",
        "  \n",
        "#### **The True Reasons behind Tokenization**\n",
        "Tokenization 을 과정을 통해 생성된 token 들은 vocabulary 를 구성하는 데에 사용이 됩니다. Vocabulary 는 주어진 corpus 에서 unique token 들의 set 을 의미하며, vocabulary 를 구성할 때 corpus 내에서 몇 번 등장하였는지 그 빈도수를 기준으로 상위 K 개의 token 들을 사용합니다.\n",
        "\n",
        "Vocabulary 는 `TF-IDF` 에서는 vocabulary 내 각 단어들이 특정 feature 로 사용이 되며, 딥러닝 계열의 모델에서는 자연어로 이루어진 입력 문장을 tokenizing 하여서 index id 로 표현하는 데에 사용이 됩니다. \n",
        "\n",
        "\n",
        "#### **Which Tokenization Should We Use?**\n",
        "어떤 Tokenization 을 사용하냐에 따라, 인공지능 모델의 성능이 달라지므로 효과적인 tokenization 방법을 사용하는 것이 중요합니다. 인공지능 모델의 성능이 달라지는 이유는 **Out of Vocabulary (OOV)** 단어들을 다루는 방법이 tokenization 방법마다 다르기 때문입니다. 보편적으로 쓰이는 Word, Character, 그리고 Subword 단위들의 방법이 OOV 문제를 다룰 때 어떤 문제가 발생하는지에 대해 설명하겠습니다.\n",
        "\n",
        "- Word Tokenization:\n",
        "  - OOV 문제를 완화하기 위해 rare word 에 대해 **unknown token** 으로 치환하여 사용하는 일종의 트릭을 사용하여 해결하고자 합니다. 해당 방법을 통해, 모델이 OOV 토큰의 representation meaning 을 학습이 가능합니다.\n",
        "  - 그러나, 모든 rare word 들에 대해 동일한 토큰인 unknown token 으로 치환을 하므로, \n",
        "    - 실질적인 OOV token 의 의미를 잃어버릴 수 있으며\n",
        "    - 동일한 representation meaning 을 지님\n",
        "\n",
        "- Character Tokenization:\n",
        "  - OOV 문제를 Word Tokenization 보다는 해결하나, 입력 문장이 길어질수록 모델의 입력으로 들어가는 문장의 길이가 길어지므로 character 간의 relationship 을 학습하기가 어려울 수 있습니다. \n",
        "\n",
        "- Subword Tokenization:\n",
        "  - Word 단위와 Character 단위의 장점만을 반영한 방법으로, 토큰 단위가 n-gram character 입니다. 대표적으로 Byte Pair Encoding (BPE) 방법이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcETfgxYyX8J"
      },
      "source": [
        "## Variants of Tokenization Model\n",
        "\n",
        "Tokenization 모델은 어떤 단위를 token 으로 사용하냐에 따라 다릅니다. 이번 시간에는 Word, Subword 단위의 Tokenization 방법들이 어떤 결과물을 나타내는지를 살펴보도록 하겠습니다. 실습을 위해 `kakaobrain` 팀의 `Pororo` 라이브러리를 활용하겠습니다.\n",
        "\n",
        "- `Pororo` 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5bJDZUzqL7"
      },
      "source": [
        "!pip install -q pororo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "7lQdZiepzgey",
        "outputId": "a89ed794-da2d-4af9-b8dc-4fc02ee26558"
      },
      "source": [
        "from pororo import Pororo\n",
        "\n",
        "Pororo.available_models('tokenization')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Available models for tokenization are ([lang]: en, [model]: moses, bpe32k.en, roberta, sent_en), ([lang]: ko, [model]: bpe4k.ko, bpe8k.ko, bpe16k.ko, bpe32k.ko, bpe64k.ko, unigram4k.ko, unigram8k.ko, unigram16k.ko, unigram32k.ko, unigram64k.ko, jpe4k.ko, jpe8k.ko, jpe16k.ko, jpe32k.ko, jpe64k.ko, mecab.bpe4k.ko, mecab.bpe8k.ko, mecab.bpe16k.ko, mecab.bpe32k.ko, mecab.bpe64k.ko, char, jamo, word, mecab_ko, sent_ko), ([lang]: ja, [model]: mecab, bpe8k.ja, sent_ja), ([lang]: zh, [model]: jieba, sent_zh)'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cyc68vT1i_G",
        "outputId": "e8a7fcf3-4408-48d8-a3ba-0de2b9a34945"
      },
      "source": [
        "!pip install python-mecab-ko"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-mecab-ko\n",
            "  Downloading python-mecab-ko-1.0.12.tar.gz (9.7 kB)\n",
            "Collecting pybind11~=2.0\n",
            "  Downloading pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 3.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-mecab-ko\n",
            "  Building wheel for python-mecab-ko (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-mecab-ko\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-mecab-ko\n",
            "Failed to build python-mecab-ko\n",
            "Installing collected packages: pybind11, python-mecab-ko\n",
            "    Running setup.py install for python-mecab-ko ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: python-mecab-ko was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed pybind11-2.7.1 python-mecab-ko-1.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0JhFlu02t1"
      },
      "source": [
        "#### **Jamo Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBHNQeNZ0_5d",
        "outputId": "19204bae-3019-410a-a9f6-1986977b4331"
      },
      "source": [
        "jamo_tok = Pororo(task='tokenization', lang='ko', model='jamo')\n",
        "jamo_result = jamo_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")\n",
        "print(jamo_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ᄉ', 'ᅳ', 'ᄑ', 'ᅡ', 'ᄋ', 'ᅵ', 'ᄃ', 'ᅥ', 'ᄆ', 'ᅢ', 'ᆫ', '▁', '3', 'ᄑ', 'ᅧ', 'ᆫ', '▁', 'ᄋ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅪ', 'ᄀ', 'ᅡ', '▁', 'ᄒ', 'ᅡ', 'ᄅ', 'ᅮ', '▁', 'ᄈ', 'ᅡ', 'ᆯ', 'ᄅ', 'ᅵ', '▁', 'ᄀ', 'ᅢ', 'ᄇ', 'ᅩ', 'ᆼ', 'ᄒ', 'ᅢ', 'ᆻ', 'ᄋ', 'ᅳ', 'ᄆ', 'ᅧ', 'ᆫ', '▁', 'ᄌ', 'ᅩ', 'ᇂ', 'ᄀ', 'ᅦ', 'ᆻ', 'ᄃ', 'ᅡ', '▁', 'ᅲ', 'ᅲ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cew7I1LN3-49"
      },
      "source": [
        "#### **Character Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB7O4fGH4Esc",
        "outputId": "2588301a-ff4c-4593-e95f-9032913ff7e4"
      },
      "source": [
        "char_tok = Pororo(task='tokenization', lang='ko', model='char')\n",
        "char_result = char_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")\n",
        "print(char_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['스', '파', '이', '더', '맨', '▁', '3', '편', '▁', '영', '화', '가', '▁', '하', '루', '▁', '빨', '리', '▁', '개', '봉', '했', '으', '면', '▁', '좋', '겠', '다', '▁', 'ㅠ', 'ㅠ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAw6SQEhzZK2"
      },
      "source": [
        "#### **Word Tokenization**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Be8LLJP0if4",
        "outputId": "2a2603c8-f23b-439d-8bf3-56a79f3e33e8"
      },
      "source": [
        "word_tok = Pororo(task='tokenization', lang='ko', model='word')\n",
        "word_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['스파이더맨', '3편', '영화가', '하루', '빨리', '개봉했으면', '좋겠다', 'ㅠㅠ']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJygJzaV1PmH"
      },
      "source": [
        "#### **Morpheme Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSk4o03K1TvE",
        "outputId": "8298edd9-7f86-4b29-c4b6-859f40044679"
      },
      "source": [
        "morp_tok = Pororo(task='tokenization', lang='ko', model='mecab_ko')\n",
        "morp_result = morp_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")\n",
        "print(morp_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['스파이더맨', ' ', '3', '편', ' ', '영화', '가', ' ', '하루', ' ', '빨리', ' ', '개봉', '했', '으면', ' ', '좋', '겠', '다', ' ', 'ㅠㅠ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyFO9OmH2f_L"
      },
      "source": [
        "#### **Subword Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G1MD6Ws2npU",
        "outputId": "dcc494eb-abbe-4744-b3e1-5ee54521bebf"
      },
      "source": [
        "subword_tok = Pororo(task='tokenization', lang='ko', model='bpe32k.ko')\n",
        "subword_result = subword_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")\n",
        "print(subword_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁스파이', '더', '맨', '▁3', '편', '▁영화가', '▁하루', '▁빨리', '▁개봉', '했으면', '▁좋겠다', '▁', 'ᅲ', 'ᅲ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlaiO23R20b8"
      },
      "source": [
        "#### **Morpheme-aware Subword Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7cwuhVr24hJ",
        "outputId": "9e0a37f0-f349-4118-c3d7-57f88f2b4995"
      },
      "source": [
        "morp_subword_tok = Pororo(task='tokenization', lang='ko', model='mecab.bpe32k.ko')\n",
        "morp_subword_result = morp_subword_tok(\"스파이더맨 3편 영화가 하루 빨리 개봉했으면 좋겠다 ㅠㅠ\")\n",
        "print(morp_subword_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁스파이더맨', '▁3', '편', '▁영화', '가', '▁하루', '▁빨리', '▁개봉', '했', '으면', '▁좋', '겠', '다', '▁', 'ㅠㅠ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hw1JNgOigyo"
      },
      "source": [
        "## 그 외, (참고용)\n",
        "\n",
        "**NLTK**\n",
        "\n",
        "NLTK (Natural Language Toolkit) 은 교육용으로 개발된 자연어 처리 및 문서분석용 파이썬 패키지입니다. 주요 기능으로는 corpus 제공, 토큰 생성, 형태소 분석, 품사 태깅 등이 있습니다.\n",
        "\n",
        "[spacy]: https://spacy.io/\n",
        "[stanford]: https://stanfordnlp.github.io/CoreNLP/\n",
        "[apache]: https://opennlp.apache.org/\n",
        "[allen]: https://allennlp.org/\n",
        "[gensim]: https://radimrehurek.com/gensim/\n",
        "[textblob]: https://textblob.readthedocs.io/en/dev/\n",
        "\n",
        "그 외 자연어 처리 분석 패키지\n",
        "- [SpaCy][spacy] \n",
        "- [Stanford Core NLP][stanford]\n",
        "- [Apache OpenNLP][apache]\n",
        "- [AllenNLP][allen]\n",
        "- [GenSim][gensim]\n",
        "- [TextBlob][textblob]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZFJvCqf7517"
      },
      "source": [
        "## Sentence Splitter (문장 분리기)\n",
        "\n",
        "문서가 긴 경우에는 문장 단위로 자르는 것이 중요합니다. 이번 시간에는 `sentsplit` 라이브러리를 활용하여 실습을 진행해보겠습니다.\n",
        "\n",
        "[sentsplit]: https://github.com/zaemyung/sentsplit\n",
        "- github repo: [sentsplit]\n",
        "\n",
        "#### **What is `sentsplit`?**\n",
        "A flexible sentence segmentation library using CRF model and regex rules.\n",
        "\n",
        "This library allows splitting of text paragraphs into sentences. It is built with the following desiderata:\n",
        "\n",
        "- Be able to extend to new languages or \"types\" of sentences from data alone by learning a conditional random field (CRF) model.\n",
        "- Also provide functionality to segment (or not to segment) lines based on regular expression rules (referred as segment_regexes and prevent_regexes, respectively).\n",
        "- Be able to reconstruct the exact original text paragraphs from joining the segmented sentences.\n",
        "\n",
        "All in all, the library aims to benefit from the best of both worlds: data-driven and rule-based approaches.\n",
        "\n",
        "#### **How to use: Python Library**\n",
        "```python\n",
        "from sentsplit.segment import SentSplit\n",
        "\n",
        "# use default setting\n",
        "sent_splitter = SentSplit(lang_code)\n",
        "\n",
        "# override default setting - see \"Features\" for detail\n",
        "sent_splitter = SentSplit(lang_code, **overriding_kwargs)\n",
        "\n",
        "# segment a single line\n",
        "sentences = sent_splitter.segment(line)\n",
        "\n",
        "# can also segment a list of lines\n",
        "sentences = sent_splitter.segment([lines])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nF29OJ8-lF1",
        "outputId": "981b41f4-7494-4b04-faa7-c10838b8a076"
      },
      "source": [
        "!pip install -q sentsplit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 743 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsK-v7sz-shZ",
        "outputId": "16ca52c4-a8e4-45d6-b085-e442fddeffe3"
      },
      "source": [
        "from copy import deepcopy\n",
        "from sentsplit.config import ko_config\n",
        "from sentsplit.segment import SentSplit\n",
        "\n",
        "w_regex_my_config = deepcopy(ko_config)\n",
        "wo_regex_my_config = deepcopy(ko_config)\n",
        "\n",
        "w_regex_my_config['segment_regexes'].append({'name': 'tilde_ending', 'regex': r'(?<=[다요])~+(?= )', 'at': 'end'})\n",
        "\n",
        "w_sent_splitter = SentSplit('ko', **w_regex_my_config)\n",
        "wo_sent_splitter = SentSplit('ko', **wo_regex_my_config)\n",
        "\n",
        "w_result = w_sent_splitter.segment('안녕하세요~ 만나서 정말 반갑습니다~~ 잘 부탁드립니다!')\n",
        "wo_result = wo_sent_splitter.segment('안녕하세요~ 만나서 정말 반갑습니다~~ 잘 부탁드립니다!')\n",
        "\n",
        "print(f'\\nWith Regex: {w_result}')\n",
        "print(f'Without Regex: {wo_result}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-08 01:23:19.457 | INFO     | sentsplit.segment:__init__:47 - SentSplit for KO loaded:\n",
            "{ 'handle_multiple_spaces': True,\n",
            "  'maxcut': 500,\n",
            "  'mincut': 5,\n",
            "  'model': 'crf_models/ko-default-05042021.model',\n",
            "  'ngram': 5,\n",
            "  'prevent_regexes': [ { 'name': 'liberal_url',\n",
            "                         'regex': '\\\\b((?:[a-z][\\\\w\\\\-]+:(?:\\\\/{1,3}|[a-z0-9%])|www\\\\d{0,3}[.]|[a-z0-9.\\\\-]+[.][a-z]{2,4}\\\\/)(?:[^\\\\s()<>]|\\\\((?:[^\\\\s()<>]|(?:\\\\([^\\\\s()<>]+\\\\)))*\\\\))+(?:\\\\((?:[^\\\\s()<>]|(?:\\\\([^\\\\s()<>]+\\\\)))*\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\\\\\'\".,<>?«»“”‘’]))'},\n",
            "                       { 'name': 'period_followed_by_lowercase',\n",
            "                         'regex': '\\\\.(?= *[a-z])'}],\n",
            "  'prevent_word_split': True,\n",
            "  'segment_regexes': [ {'at': 'end', 'name': 'after_semicolon', 'regex': ' *;'},\n",
            "                       { 'at': 'end',\n",
            "                         'name': 'ellipsis',\n",
            "                         'regex': '…(?![\\\\!\\\\?\\\\.．？！])'},\n",
            "                       {'at': 'end', 'name': 'newline', 'regex': '\\\\n'},\n",
            "                       { 'at': 'end',\n",
            "                         'name': 'tilde_ending',\n",
            "                         'regex': '(?<=[다요])~+(?= )'}],\n",
            "  'strip_spaces': False}\n",
            "2021-09-08 01:23:19.462 | INFO     | sentsplit.segment:__init__:47 - SentSplit for KO loaded:\n",
            "{ 'handle_multiple_spaces': True,\n",
            "  'maxcut': 500,\n",
            "  'mincut': 5,\n",
            "  'model': 'crf_models/ko-default-05042021.model',\n",
            "  'ngram': 5,\n",
            "  'prevent_regexes': [ { 'name': 'liberal_url',\n",
            "                         'regex': '\\\\b((?:[a-z][\\\\w\\\\-]+:(?:\\\\/{1,3}|[a-z0-9%])|www\\\\d{0,3}[.]|[a-z0-9.\\\\-]+[.][a-z]{2,4}\\\\/)(?:[^\\\\s()<>]|\\\\((?:[^\\\\s()<>]|(?:\\\\([^\\\\s()<>]+\\\\)))*\\\\))+(?:\\\\((?:[^\\\\s()<>]|(?:\\\\([^\\\\s()<>]+\\\\)))*\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\\\\\'\".,<>?«»“”‘’]))'},\n",
            "                       { 'name': 'period_followed_by_lowercase',\n",
            "                         'regex': '\\\\.(?= *[a-z])'}],\n",
            "  'prevent_word_split': True,\n",
            "  'segment_regexes': [ {'at': 'end', 'name': 'after_semicolon', 'regex': ' *;'},\n",
            "                       { 'at': 'end',\n",
            "                         'name': 'ellipsis',\n",
            "                         'regex': '…(?![\\\\!\\\\?\\\\.．？！])'},\n",
            "                       {'at': 'end', 'name': 'newline', 'regex': '\\\\n'}],\n",
            "  'strip_spaces': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "With Regex: ['안녕하세요~', ' 만나서 정말 반갑습니다~~', ' 잘 부탁드립니다!']\n",
            "Without Regex: ['안녕하세요~ 만나서 정말 반갑습니다~~', ' 잘 부탁드립니다!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVFC4u4GlMzn"
      },
      "source": [
        "#### **그 외,**\n",
        "\n",
        "- `sent_tokenize` 함수는 말뭉치를 문장 단위로 토큰화 역할을 수행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEH5cM0GifWK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6e72303-11a5-4cda-86ee-5902ff5cf8c9"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. And you are a Ph.D student.\"\n",
        "\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llt78mhP5peU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}