{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_KoBERT_dialect_dataset_label_generation_kw_70000.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQXbdHRYim3AxaO330gTmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrotherKim/KAIST_CODE/blob/master/SEP531/term/hak_1_KoBERT_dialect_dataset_label_generation_kw_70000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYdNv9u_wZ4",
        "outputId": "cf57a5ca-1a77-45c9-8926-444bb0d1b2b2"
      },
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FPEfwJNuqA"
      },
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled'\n",
        "TRAIN_DIR = '%s/train' % ROOT_DIR\n",
        "VALID_DIR = '%s/valid' % ROOT_DIR\n",
        "\n",
        "NUM_TRAIN = 171\n",
        "NUM_VALID = 58\n",
        "\n",
        "SAVEPOINT_PATH = '/content/drive/MyDrive/KAIST/SEP531/KoBERT_emotion_70000_std.pt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9TKL9NZ_Dh-"
      },
      "source": [
        "#실행 환경(사용)#\n",
        "\n",
        "- Python >= 3.6\n",
        "- PyTorch >= 1.70\n",
        "- Transformers = 3.0.2\n",
        "- Colab\n",
        "- batch size = 64 (convertable)\n",
        "- epochs = 10 (convertable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU6f6roY-5sW",
        "outputId": "0575617e-f428-4340-bae6-f35faeec8662"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.6)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595734 sha256=60fa8539e66ab0656eb43a6fb94051ab8b78333d2cbf4efe8327ea1e919649c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlNTmU-_WS9"
      },
      "source": [
        "#KoBERT 다운로드(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12JYVZJ_al4",
        "outputId": "935f7603-2452-439f-8021-ca669703a29a"
      },
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-2yuxuj1o\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-2yuxuj1o\n",
            "Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.10.0)\n",
            "Requirement already satisfied: mxnet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.8.0.post0)\n",
            "Collecting onnxruntime>=0.3.0\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (21.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (0.29.24)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (0.8.4)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (3.17.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.1.2) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.8.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 522 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.1.2) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.1.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime>=0.3.0->kobert==0.1.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (1.1.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=13136 sha256=b704c748d2baed2071a6386b2f2995a72b7cbf495326d35eecbce0e705b98470\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t16kvif1/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, onnxruntime, kobert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed huggingface-hub-0.2.1 kobert-0.1.2 onnxruntime-1.10.0 pyyaml-6.0 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG56WkzA_ccN"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enK_3xLH_ecb"
      },
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x35BKT7q_gAU"
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoVIALyi_hR_",
        "outputId": "e130df30-d263-4826-fa7f-2036da2ab2f6"
      },
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            ".cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUtyPARZBxUm"
      },
      "source": [
        "#KoBERT 입력 데이터로 변환(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56vNLMjoB0K7"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Cn--xmB2Kg"
      },
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAOyPXXC2_x"
      },
      "source": [
        "#원본 데이터셋으로부터 데이터 추출하기#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpuTUhB2Npyo"
      },
      "source": [
        "# 터미널 커맨드를 문자열로 반환해주는 함수 작성\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def GetShellCmdStdOut(command):\n",
        "  cmd = ['sh', '-c', command]\n",
        "  fd_popen = subprocess.Popen(cmd, stdout=subprocess.PIPE).stdout \n",
        "  data = fd_popen.read().strip() \n",
        "  fd_popen.close()\n",
        "\n",
        "  retval = data.decode('utf-8') \n",
        "  return retval"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxMSGeurHAkO",
        "outputId": "df5d11a0-211d-4fdf-a7e8-dc9f45b31680"
      },
      "source": [
        "!echo $VALID_DIR"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX4CsLP7LvGO",
        "outputId": "239de725-458b-4a25-fa4d-cbf40732006f"
      },
      "source": [
        "%pushd $TRAIN_DIR\n",
        "tl = GetShellCmdStdOut('find | grep json | grep -v data | grep -v answer| grep -v config').split('\\n')\n",
        "train_json_list = [x.replace('./', '') for x in tl]\n",
        "train_json_list.sort()\n",
        "%popd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/train\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGRX2OgDkKtC",
        "outputId": "d9e5c3ba-fe7d-4733-b29d-ac76c0ef2fa2"
      },
      "source": [
        "%pushd $VALID_DIR\n",
        "vl = GetShellCmdStdOut('find | grep json | grep -v data | grep -v answer | grep -v config').split('\\n')\n",
        "valid_json_list = [x.replace('./', '') for x in vl]\n",
        "valid_json_list.sort()\n",
        "%popd"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y5Il9xoPztk",
        "outputId": "36b9af87-c8b4-4288-9bea-d574c40b9432"
      },
      "source": [
        "len(train_json_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4717"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ax21CMQI9Uw",
        "outputId": "56ba7c9f-fc1f-4d25-e157-0d635b0456fa"
      },
      "source": [
        "len(valid_json_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "828"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH_DSjp7CXW9"
      },
      "source": [
        "#추출한 데이터에 감정 레이블 추가(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZ_QCKLCKnG"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zowlRfd3Cduu"
      },
      "source": [
        "새로운 문장을 테스트 할 때, 입력되는 문장을 KoBERT의 입력 형식으로 바꿔주는 코드를 작성해주어야 한다. 아래 코드를 작성하여 토큰화, 패딩, 텐서를 바꿔주고 예측을 하는 'predict' 함수를 만들어 주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Mlpc2DHjXb",
        "outputId": "5645bc9c-6688-4c5b-ad60-e24a351f15a8"
      },
      "source": [
        "#모델 불러오기\n",
        "model1 = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "model1.load_state_dict(torch.load(SAVEPOINT_PATH), strict=False)\n",
        "model1.eval()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAROeR5XCW0d",
        "outputId": "e744c51f-27cc-423a-b400-bfb5d244f187"
      },
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "dic = {0:'공포', 1:'놀람', 2:'분노', 3:'슬픔', 4:'중립', 5:'행복', 6:'혐오'}\n",
        "def getEmotion(out):\n",
        "  idx_eval=[]\n",
        "  val_eval=[]\n",
        "  for i in out:\n",
        "    logits=i\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    idx = np.argmax(logits)\n",
        "    idx_eval.append(idx)\n",
        "    val_eval.append(dic.get(idx))\n",
        "    return idx_eval[0], val_eval[0]\n",
        "\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model1.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model1(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        return getEmotion(out)\n",
        "\n",
        "\n",
        "        # file wrting"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2rDG1Tane3j"
      },
      "source": [
        "import csv\n",
        "\n",
        "def GenLabelFromData70000(path, jsonfilename):\n",
        "  try:\n",
        "    fin = open('%s/%s.data' % (path, jsonfilename), 'r')\n",
        "  except:\n",
        "    print('No file(%s/%s.data)' % (path, jsonfilename))\n",
        "    return\n",
        "\n",
        "  with open('%s/%s.data' % (path, jsonfilename), 'r') as f:\n",
        "    with open('%s/%s.answer70000' % (path, jsonfilename), 'w') as o:\n",
        "      csv_data = csv.reader(f, delimiter='\\t')\n",
        "      for i, line in enumerate(csv_data):\n",
        "        std = line[0]\n",
        "        dia = line[1]\n",
        "        idx, val = predict(line[0])\n",
        "        #print('%s(%d) %s\\n' % (val, idx, std))\n",
        "        #print('[%d]%s/%s/%d/%s\\n' % (i, std, dia, idx, val))\n",
        "        o.write('%s\\t%s\\t%d\\n' % (std, dia, idx))\n",
        "  print('>> Generated file %s/%s.answer\\n' % (path, jsonfilename))\n",
        "\n",
        "  #with open('%s/%s.data' % (path, jsonfilename), 'w') as f:\n",
        "  #  for u in ut:\n",
        "  #    f.write('%s\\t%s\\n' % (u['standard_form'], u['dialect_form']))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LK5f-dczr3U",
        "outputId": "56041271-1d9a-48d3-ae4a-ea34da49921a"
      },
      "source": [
        "train_json_list[0:171]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DGDQ20000020.json',\n",
              " 'DGDQ20000027.json',\n",
              " 'DGDQ20000029.json',\n",
              " 'DGDQ20000032.json',\n",
              " 'DGDQ20000034.json',\n",
              " 'DGDQ20000036.json',\n",
              " 'DGDQ20000045.json',\n",
              " 'DGDQ20000047.json',\n",
              " 'DGDQ20000053.json',\n",
              " 'DGDQ20000061.json',\n",
              " 'DGDQ20000064.json',\n",
              " 'DGDQ20000067.json',\n",
              " 'DGDQ20000068.json',\n",
              " 'DGDQ20000070.json',\n",
              " 'DGDQ20000071.json',\n",
              " 'DGDQ20000072.json',\n",
              " 'DGDQ20000077.json',\n",
              " 'DGDQ20000078.json',\n",
              " 'DGDQ20000082.json',\n",
              " 'DGDQ20000083.json',\n",
              " 'DGDQ20000084.json',\n",
              " 'DGDQ20000088.json',\n",
              " 'DGDQ20000089.json',\n",
              " 'DGDQ20000090.json',\n",
              " 'DGDQ20000094.json',\n",
              " 'DGDQ20000098.json',\n",
              " 'DGDQ20000099.json',\n",
              " 'DGDQ20000102.json',\n",
              " 'DGDQ20000103.json',\n",
              " 'DGDQ20000106.json',\n",
              " 'DGDQ20000109.json',\n",
              " 'DGDQ20000110.json',\n",
              " 'DGDQ20000111.json',\n",
              " 'DGDQ20000115.json',\n",
              " 'DGDQ20000119.json',\n",
              " 'DGDQ20000120.json',\n",
              " 'DGDQ20000121.json',\n",
              " 'DGDQ20000122.json',\n",
              " 'DGDQ20000123.json',\n",
              " 'DGDQ20000124.json',\n",
              " 'DGDQ20000125.json',\n",
              " 'DGDQ20000131.json',\n",
              " 'DGDQ20000132.json',\n",
              " 'DGDQ20000135.json',\n",
              " 'DGDQ20000138.json',\n",
              " 'DGDQ20000139.json',\n",
              " 'DGDQ20000144.json',\n",
              " 'DGDQ20000145.json',\n",
              " 'DGDQ20000146.json',\n",
              " 'DGDQ20000156.json',\n",
              " 'DGDQ20000157.json',\n",
              " 'DGDQ20000162.json',\n",
              " 'DGDQ20000163.json',\n",
              " 'DGDQ20000164.json',\n",
              " 'DGDQ20000165.json',\n",
              " 'DGDQ20000169.json',\n",
              " 'DGDQ20000170.json',\n",
              " 'DGDQ20000171.json',\n",
              " 'DGDQ20000177.json',\n",
              " 'DGDQ20000178.json',\n",
              " 'DGDQ20000181.json',\n",
              " 'DGDQ20000189.json',\n",
              " 'DGDQ20000190.json',\n",
              " 'DGDQ20000196.json',\n",
              " 'DGDQ20000201.json',\n",
              " 'DGDQ20000202.json',\n",
              " 'DGDQ20000203.json',\n",
              " 'DGDQ20000218.json',\n",
              " 'DGDQ20000227.json',\n",
              " 'DGDQ20000228.json',\n",
              " 'DGDQ20000255.json',\n",
              " 'DGDQ20000283.json',\n",
              " 'DGDQ20000284.json',\n",
              " 'DGDQ20000345.json',\n",
              " 'DGDQ20000347.json',\n",
              " 'DGDQ20000386.json',\n",
              " 'DGDQ20000388.json',\n",
              " 'DGDQ20000397.json',\n",
              " 'DGDQ20000398.json',\n",
              " 'DGDQ20000402.json',\n",
              " 'DGDQ20000411.json',\n",
              " 'DGDQ20000417.json',\n",
              " 'DGDQ20000432.json',\n",
              " 'DGDQ20000448.json',\n",
              " 'DGDQ20000481.json',\n",
              " 'DGDQ20000483.json',\n",
              " 'DGDQ20000493.json',\n",
              " 'DGDQ20000494.json',\n",
              " 'DGDQ20000495.json',\n",
              " 'DGDQ20000497.json',\n",
              " 'DGDQ20000507.json',\n",
              " 'DGDQ20000509.json',\n",
              " 'DGDQ20000530.json',\n",
              " 'DGDQ20000554.json',\n",
              " 'DGDQ20000556.json',\n",
              " 'DGDQ20000562.json',\n",
              " 'DGDQ20000578.json',\n",
              " 'DGDQ20000579.json',\n",
              " 'DGDQ20000586.json',\n",
              " 'DGDQ20000604.json',\n",
              " 'DGDQ20000609.json',\n",
              " 'DGDQ20000641.json',\n",
              " 'DGDQ20000655.json',\n",
              " 'DGDQ20001002.json',\n",
              " 'DGDQ20001003.json',\n",
              " 'DGDQ20001004.json',\n",
              " 'DGDQ20001005.json',\n",
              " 'DGDQ20001011.json',\n",
              " 'DGDQ20001021.json',\n",
              " 'DGDQ20001025.json',\n",
              " 'DGDQ20001029.json',\n",
              " 'DGDQ20001030.json',\n",
              " 'DGDQ20001034.json',\n",
              " 'DGDQ20001035.json',\n",
              " 'DGDQ20001036.json',\n",
              " 'DGDQ20001037.json',\n",
              " 'DGDQ20001038.json',\n",
              " 'DGDQ20001039.json',\n",
              " 'DGDQ20001041.json',\n",
              " 'DGDQ20001042.json',\n",
              " 'DGDQ20001045.json',\n",
              " 'DGDQ20001046.json',\n",
              " 'DGDQ20001047.json',\n",
              " 'DGDQ20001048.json',\n",
              " 'DGDQ20001049.json',\n",
              " 'DGDQ20001050.json',\n",
              " 'DGDQ20001053.json',\n",
              " 'DGDQ20001055.json',\n",
              " 'DGDQ20001059.json',\n",
              " 'DGDQ20001060.json',\n",
              " 'DGDQ20001061.json',\n",
              " 'DGDQ20001063.json',\n",
              " 'DGDQ20001064.json',\n",
              " 'DGDQ20001067.json',\n",
              " 'DGDQ20001069.json',\n",
              " 'DGDQ20001070.json',\n",
              " 'DGDQ20001071.json',\n",
              " 'DGDQ20001072.json',\n",
              " 'DGDQ20001073.json',\n",
              " 'DGDQ20001074.json',\n",
              " 'DGDQ20001075.json',\n",
              " 'DGDQ20001076.json',\n",
              " 'DGDQ20001077.json',\n",
              " 'DGDQ20001078.json',\n",
              " 'DGDQ20001079.json',\n",
              " 'DGDQ20001080.json',\n",
              " 'DGDQ20001081.json',\n",
              " 'DGDQ20001083.json',\n",
              " 'DGDQ20001084.json',\n",
              " 'DGDQ20001086.json',\n",
              " 'DGDQ20001087.json',\n",
              " 'DGDQ20001101.json',\n",
              " 'DGDQ20001107.json',\n",
              " 'DGDQ20001108.json',\n",
              " 'DGDQ20001109.json',\n",
              " 'DGDQ20001110.json',\n",
              " 'DGDQ20001111.json',\n",
              " 'DGDQ20001114.json',\n",
              " 'DGDQ20001115.json',\n",
              " 'DGDQ20001116.json',\n",
              " 'DGDQ20001117.json',\n",
              " 'DGDQ20001118.json',\n",
              " 'DGDQ20001123.json',\n",
              " 'DGDQ20001127.json',\n",
              " 'DGDQ20001128.json',\n",
              " 'DGDQ20001141.json',\n",
              " 'DGDQ20001144.json',\n",
              " 'DGDQ20001146.json',\n",
              " 'DGDQ20001147.json',\n",
              " 'DGDQ20001171.json',\n",
              " 'DGDQ20001182.json']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_json_list[0:58]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iO690ueczGs",
        "outputId": "e4333435-07b5-4752-aa1f-6c31b8c29ec5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DGIN21500168.json',\n",
              " 'DGIN21500169.json',\n",
              " 'DGIN21500170.json',\n",
              " 'DGIN21500171.json',\n",
              " 'DGIN21500172.json',\n",
              " 'DGIN21500173.json',\n",
              " 'DGIN21500174.json',\n",
              " 'DGIN21500175.json',\n",
              " 'DGIN21500176.json',\n",
              " 'DGIN21500177.json',\n",
              " 'DGIN21500179.json',\n",
              " 'DGIN21500180.json',\n",
              " 'DGIN21500181.json',\n",
              " 'DGIN21500182.json',\n",
              " 'DGIN21500183.json',\n",
              " 'DGIN21500184.json',\n",
              " 'DGIN21500185.json',\n",
              " 'DGIN21500186.json',\n",
              " 'DGIN21500187.json',\n",
              " 'DGIN21500188.json',\n",
              " 'DGIN21500189.json',\n",
              " 'DGIN21500190.json',\n",
              " 'DGIN21500191.json',\n",
              " 'DGIN21500192.json',\n",
              " 'DGIN21500193.json',\n",
              " 'DGIN21500194.json',\n",
              " 'DGIN21500195.json',\n",
              " 'DGIN21500196.json',\n",
              " 'DGIN21500197.json',\n",
              " 'DGIN21500198.json',\n",
              " 'DGIN21500199.json',\n",
              " 'DGIN21500200.json',\n",
              " 'DGIN21500201.json',\n",
              " 'DGIN21500202.json',\n",
              " 'DGIN21500203.json',\n",
              " 'DGIN21500204.json',\n",
              " 'DGIN21500205.json',\n",
              " 'DGIN21500206.json',\n",
              " 'DGIN21500207.json',\n",
              " 'DGIN21500208.json',\n",
              " 'DGIN21500209.json',\n",
              " 'DGIN21500210.json',\n",
              " 'DGIN21500211.json',\n",
              " 'DGIN21500212.json',\n",
              " 'DGIN21500213.json',\n",
              " 'DGIN21500214.json',\n",
              " 'DGIN21500215.json',\n",
              " 'DGIN21500216.json',\n",
              " 'DGIN21500217.json',\n",
              " 'DGIN21500218.json',\n",
              " 'DGIN21500219.json',\n",
              " 'DGIN21500220.json',\n",
              " 'DGIN21500221.json',\n",
              " 'DGIN21500222.json',\n",
              " 'DGIN21500223.json',\n",
              " 'DGIN21500224.json',\n",
              " 'DGIN21500225.json',\n",
              " 'DGIN21500226.json']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for문을 돌면서 json 파일 내에 있는 표준어 문장과 사투리 문장을 추출해서 FILENAME.data로 저장\n",
        "import json\n",
        "\n",
        "def GenDataFromJson(path, jsonfilename):\n",
        "  with open('%s/%s' % (path, jsonfilename), 'r') as f:\n",
        "    json_data = json.load(f);\n",
        "  #print(json.dumps(json_data, indent='\\t'))\n",
        "  ut = json_data['utterance']\n",
        "\n",
        "  with open('%s/%s.data' % (path, jsonfilename), 'w') as f:\n",
        "    for u in ut:\n",
        "      f.write('%s\\t%s\\n' % (u['standard_form'], u['dialect_form']))\n",
        "   \n",
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 * 350개 문장\n",
        "#for f in train_json_list[0:171]:\n",
        "  #GenDataFromJson(TRAIN_DIR, f)\n",
        "%popd\n",
        "\n",
        "%pushd $VALID_DIR\n",
        "# 150개 파일 350개 문장\n",
        "#for f in valid_json_list[0:58]:\n",
        "  #GenDataFromJson(VALID_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8V7suhMCeLH",
        "outputId": "0a165995-6521-4c75-9317-b0d59111c4f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/train\n",
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n",
            "popd -> /content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n",
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n",
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n",
            "popd -> /content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNvc_w1DoB0g",
        "outputId": "8dcb46e4-ac2e-4ba3-e4c6-13bc7ed217be"
      },
      "source": [
        "#%pushd $TRAIN_DIR\n",
        "## 450개 파일 350개 문장\n",
        "#for i, f in enumerate(train_json_list[0:100]):\n",
        "#  print('[%d][%s]' % (i, f))\n",
        "#  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "#%popd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_kw_labeled/train\n",
            "[0][DGDQ20000020.json]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000020.json.answer\n",
            "\n",
            "[1][DGDQ20000027.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000027.json.answer\n",
            "\n",
            "[2][DGDQ20000029.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000029.json.answer\n",
            "\n",
            "[3][DGDQ20000032.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000032.json.answer\n",
            "\n",
            "[4][DGDQ20000034.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000034.json.answer\n",
            "\n",
            "[5][DGDQ20000036.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000036.json.answer\n",
            "\n",
            "[6][DGDQ20000045.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000045.json.answer\n",
            "\n",
            "[7][DGDQ20000047.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000047.json.answer\n",
            "\n",
            "[8][DGDQ20000053.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000053.json.answer\n",
            "\n",
            "[9][DGDQ20000061.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000061.json.answer\n",
            "\n",
            "[10][DGDQ20000064.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000064.json.answer\n",
            "\n",
            "[11][DGDQ20000067.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000067.json.answer\n",
            "\n",
            "[12][DGDQ20000068.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000068.json.answer\n",
            "\n",
            "[13][DGDQ20000070.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000070.json.answer\n",
            "\n",
            "[14][DGDQ20000071.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000071.json.answer\n",
            "\n",
            "[15][DGDQ20000072.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000072.json.answer\n",
            "\n",
            "[16][DGDQ20000077.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000077.json.answer\n",
            "\n",
            "[17][DGDQ20000078.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000078.json.answer\n",
            "\n",
            "[18][DGDQ20000082.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_kw_labeled/train/DGDQ20000082.json.answer\n",
            "\n",
            "[19][DGDQ20000083.json]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(train_json_list[17:100]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "id": "2l37s8uKqit3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXPq8G1PxaAp"
      },
      "source": [
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(train_json_list[101:171]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "%popd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pushd $VALID_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(valid_json_list[0:58]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(VALID_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "id": "dae6aBlqBPPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}