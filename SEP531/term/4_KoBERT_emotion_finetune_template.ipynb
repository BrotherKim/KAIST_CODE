{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrotherKim/KAIST_CODE/blob/master/SEP531/term/4_KoBERT_emotion_finetune_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEDjjC2Q-Znv"
      },
      "source": [
        "#변수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQBjtGo_-ZDe"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/KAIST/SEP531/kor_jr_labeled'\n",
        "TRAIN_DIR = '%s/train' % ROOT_DIR\n",
        "VALID_DIR = '%s/valid' % ROOT_DIR\n",
        "\n",
        "NUM_TRAIN = 171\n",
        "NUM_VALID = 58\n",
        "\n",
        "SAVEPOINT_PATH = '/content/drive/MyDrive/KAIST/SEP531/KoBERT_emotion_finetuned_data_jr.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9TKL9NZ_Dh-"
      },
      "source": [
        "#실행 환경#\n",
        "\n",
        "- Python >= 3.6\n",
        "- PyTorch >= 1.70\n",
        "- Transformers = 3.0.2\n",
        "- Colab\n",
        "- batch size = 64 (convertable)\n",
        "- epochs = 10 (convertable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU6f6roY-5sW",
        "outputId": "1c07e1f8-ec17-435e-9527-0fea89e5b1f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Collecting transformers==3.0.2\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Using cached tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.13.0\n",
            "    Uninstalling transformers-4.13.0:\n",
            "      Successfully uninstalled transformers-4.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobert 0.1.2 requires transformers>=4.8.1, but you have transformers 3.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlNTmU-_WS9"
      },
      "source": [
        "#KoBERT 다운로드#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12JYVZJ_al4",
        "outputId": "cd5eeefc-5761-469c-ffd7-98508aad3714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-lsgo0jb0\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-lsgo0jb0\n",
            "Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.10.0)\n",
            "Requirement already satisfied: mxnet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.8.0.post0)\n",
            "Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Using cached transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (21.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (0.29.24)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (2.23.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (3.17.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.1.2) (3.10.0.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.1.2) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.1.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime>=0.3.0->kobert==0.1.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (1.1.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed tokenizers-0.10.3 transformers-4.13.0\n"
          ]
        }
      ],
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RG56WkzA_ccN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enK_3xLH_ecb"
      },
      "outputs": [],
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x35BKT7q_gAU"
      },
      "outputs": [],
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoVIALyi_hR_",
        "outputId": "4f1ca664-b3f7-433f-9dc0-b8534f6f9762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ]
        }
      ],
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n3DFp5u_uub"
      },
      "source": [
        "#데이터셋 전처리#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYdNv9u_wZ4",
        "outputId": "79687136-321e-4492-8b62-db455f3ad9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2pjNGETI-AI-"
      },
      "outputs": [],
      "source": [
        "# 터미널 커맨드를 문자열로 반환해주는 함수 작성\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def GetShellCmdStdOut(command):\n",
        "  cmd = ['sh', '-c', command]\n",
        "  fd_popen = subprocess.Popen(cmd, stdout=subprocess.PIPE).stdout \n",
        "  data = fd_popen.read().strip() \n",
        "  fd_popen.close()\n",
        "\n",
        "  retval = data.decode('utf-8') \n",
        "  return retval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92CWOFVi-Cae",
        "outputId": "10a62627-65b0-48bf-8ebd-f7e7b3cebb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KAIST/SEP531/kor_jr_labeled/train\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ],
      "source": [
        "%pushd $TRAIN_DIR\n",
        "tl = GetShellCmdStdOut('find | grep answer').split('\\n')\n",
        "train_json_list = [x.replace('./', '') for x in tl]\n",
        "train_json_list.sort()\n",
        "%popd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I2BOvecE-DuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6012de-3155-47d9-9d53-049346c5905b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KAIST/SEP531/kor_jr_labeled/valid\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ],
      "source": [
        "%pushd $VALID_DIR\n",
        "vl = GetShellCmdStdOut('find | grep answer').split('\\n')\n",
        "valid_json_list = [x.replace('./', '') for x in vl]\n",
        "valid_json_list.sort()\n",
        "%popd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_json_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UA8zdpPm_olM",
        "outputId": "fa4b2aec-7b13-4264-9ac9-d2733be555f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DJSX20003906.json.answer'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $TRAIN_DIR/{train_json_list[0]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdNya_bh_r6K",
        "outputId": "54609c95-7695-4421-c536-ebe6929ae20e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "언니 근데 언니 올해 들어서 되게 머리 스타일에 관심이 많아진 거 같애.\t언니 근데 언니 올해 들어서 되게 머리 스타일에 관심이 많아진 거 같애.\t3\n",
            "어 나 왜 그랬냐면은 올해 우리 아가씨야 우리 아가씨가 그러는 거야.\t어 나 왜 그랬냐면은 올해 우리 아가씨야 우리 아가씨가 그러는 거야.\t3\n",
            "나한테 언니 언니 머리 너무 까만 색 아니야?\t나한테 언니 언니 머리 너무 까만 색 아니야?\t0\n",
            "이렇게 하는 거야 그래가지고 내가\t이렇게 하는 거야 그래가지고 내가\t3\n",
            "왜요 이렇게 했더니 언니 여름에는 그렇게 까만색 하면 더워보여 이렇게 하는거야.\t왜요 이렇게 했더니 언니 여름에는 그렇게 까만색 하면 더워보여 이렇게 하는거야.\t2\n",
            "그래서 내가\t그래서 내가\t4\n",
            "너무 짜증이 나는 거지 나보다 못생긴 애한테 그런 소리 들으니까 그래서 내가\t너무 짜증이 나는 거지 나보다 못생긴 애한테 그런 소리 들으니까 그래서 내가\t6\n",
            "아차 싶어가지고 언니랑 같이 미용실을 가기 시작했어.\t아차 싶어가지고 언니랑 같이 미용실을 가기 시작했어.\t3\n",
            "요즘 언니 머리 색깔을 보면은 무지개 후려쳐.\t요즘 언니 머리 색깔을 보면은 무지개 후려쳐.\t6\n",
            "그리고 나도 옛날에 왕년에 염색 조금 했었는데 요즘 언니 머리 색깔이 더 눈에 띄고\t글고 나도 옛날에 왕년에 염색 쫌 했었는데 요즘 언니 머리 색깔이 더 눈에 띄고\t4\n",
            "뭔 고딩(같어.)/(같아.)\t뭔 고딩(같어.)/(같아.)\t3\n",
            "아니 내가 이 머리 색깔 할려고 한 게 아니야 내 머리 색깔이 원래 애쉬 애쉬 애쉬 애쉬그레이라고 있어.\t아니 내가 이 머리 색깔 할려고 한 게 아니야 내 머리 색깔이 원래 애쉬 애쉬 애쉬 애쉬그레이라고 있어.\t4\n",
            "애쉬 그레이라고 애쉬 그레이라고 있는데\t애쉬 그레이라고 애쉬 그레이라고 있는데\t4\n",
            "이 애쉬그레이는 탈색을 열 두 번은 넘게 해야지 색깔이 나온다는 거야.\t이 애쉬그레이는 탈색을 열 두 번은 넘게 해야지 색깔이 나온다는 거야.\t4\n",
            "그러면은\t그러면은\t0\n",
            "에쉬 그레이 가 아니라 에이씨 그레이네.\t에쉬 그레이 가 아니라 에이씨 그레이네.\t4\n",
            "어 이게 완전 에이씨 그레이야 그래가지고 이제 이걸 할려고 하니까\t어 이게 완전 에이씨 그레이야 그래가지고 이제 이걸 할려고 하니까\t1\n",
            "무슨 탈색을 열 두 번 해야 되고 뭘 해야되고 해 근데 탈색 한 번 하는게\t무슨 탈색을 열 두 번 해야 되고 뭘 해야되고 해 근데 탈색 한 번 하는게\t4\n",
            "머리가 진짜 너무 따갑고\t머리가 진짜 너무 따갑고\t6\n",
            "너무 아파 그래서 나는 할수 없다 이렇게 말을 했지.\t너무 아파 그래서 나는 할수 없다 이렇게 말을 했지.\t3\n",
            "그랬더니 그러면은 이제 미용실 언니가 나한테 그러는 거야 원래 내 머리가 엄청 까만색이 었거든 알지?\t그랬더니 그러면은 이제 미용실 언니가 나한테 그러는 거야 원래 내 머리가 엄청 까만색이 었거든 알지?\t2\n",
            "어 그렇지 우리 한국인이 모르고 다 까맣지 그럼 노래?\t어 글지 우리 한국인이 모르고 다 꺼멓지 그럼 누래?\t6\n",
            "나는 근데 그 검정색으로 염색을 한 번 했었어가지고 머리가 진짜 아예 개 까망이였어 그래 가지고는\t나는 근데 그 검정색으로 염색을 한 번 했었어가지고 머리가 진짜 아예 개 까망이였어 그래 가지고는\t6\n",
            "이 머리를 어떻게 하면 이 색깔 할 수 있을까요 하니깐 그럼이 머리를 최대한 노오랗게 빼제.\t이 머리를 어떻게 하면 이 색깔 할 수 있을까요 하니깐 그럼이 머리를 최대한 노오랗게 빼제.\t6\n",
            "노랗게 노랗게 그래서 한번 뺐어.\t노랗게 노랗게 그래서 한번 뺐어.\t6\n",
            "한 번 뺐는데 머리가 노랑이 아니라 갈색인 거지.\t한 번 뺐는데 머리가 노랑이 아니라 갈색인 거지.\t4\n",
            "아니 근데 원래 검정색을 한번 뺐으니까 갈색이 될만 하지 어떻게 검정색을 한 번 뺐다고 노랑이돼.\t아니 근데 원래 검정색을 한번 뺐으니까 갈색이 될만 하지 어떻게 검정색을 한 번 뺐다고 노랑이돼.\t6\n",
            "한 두 번 빼야지 노랑 되는 거 아니야?\t한 두 번 빼야지 노랑 되는 거 아니야?\t6\n",
            "그래서 내가 칠월달에 한번 빼더니 갈색이 됐어 그래서 그다음 달이 됐어.\t그래서 내가 칠월달에 한번 빼더니 갈색이 됐어 그래서 그다음 달이 됐어.\t4\n",
            "내가 근데 내 웃긴 게 뭐냐면 내가 머리가 엄청 빨리 길어. 그래가지고\t내가 근데 내 웃긴 게 뭐냐면 내가 머리가 엄청 빨리 길어. 그래가지고\t1\n",
            "이주 길면 검정 머리가 막 일센치씩 나와버려 지금도 그래 보이지?\t이주 길면 껌정 머리가 막 일센치씩 나와부러 지금도 그래 보이지?\t4\n",
            "어\t어\t4\n",
            "일 센치 씩 나와버려 그래갖고 내가 이거 미용실(()) 선생님한테 물어봤는데\t일 센치 씩 나와부러 그래갖고 내가 이거 미용실(()) 선생님한테 물어봤는데\t3\n",
            "나 같은 스 (())\t나 같은 스 (())\t3\n",
            "스케일이 스케일이 있데 그러면서 머리가\t스케일이 스케일이 있데 그러면서 머리가\t5\n",
            "빨리 긴데 근데 내가 아는 언니가 나보고 야한 생각 그만하라는 거야 하는 생각 많이 하면 머리 기니까\t빨리 긴데 근데 내가 아는 언니가 나보고 야한 생각 그만하라는 거야 하는 생각 많이 하면 머리 기니까\t0\n",
            "(()) 나 보고 야한생각 그만하래 근데 나는 야한 생각을 일도 안 하거든 근데 머리가 이렇게 잘 길어.\t(()) 나 보고 야한생각 그만하래 근데 나는 야한 생각을 일도 안 하거든 근데 머리가 이렇게 잘 길어.\t1\n",
            "그래가지고 내가 또 그 선생님한테 가가지고 선생님 머리가 너무 자주 길어요 이렇게 하니까\t그래가지고 내가 또 그 선생님한테 가가지고 선생님 머리가 너무 자주 길어요 이렇게 하니까\t0\n",
            "뿌리 염색하시고 이렇게 하시면 돼요 이렇게 하더라고 그래가지고\t뿌리 염색하시고 이렇게 하시면 돼요 이렇게 하더라고 그래가지고\t3\n",
            "팔 월 달이 됐어 팔월달이 돼가지고 머리 한 이 센치 자라가지고 갔어 뿌염색하러\t팔 월 달이 됐어 팔월달이 돼가지고 머리 한 이 센치 자라가지고 갔어 뿌염색하러\t5\n",
            "그래가꼬 뿌염색했어 뿌염색했는데 머리가 갈색이니까 염색 한 거 같지도 않고 안 할 거 같은 거야 그래가지구 선생님한테\t그래가꼬 뿌염색했어 뿌염색했는데 머리가 갈색이니까 염색 한 거 같지도 않고 안 할 거 같은 거야 그래가지구 선생님한테\t3\n",
            "선생님 저 그냥 올 염색할래요 이래가꼬 노랑머리를 한 번 더 했어.\t선생님 저 그냥 올 염색할래요 이래가꼬 노랑머리를 한 번 더 했어.\t3\n",
            "그랬더니 이제 연갈색이 돼.\t그랬더니 이제 연갈색이 돼.\t6\n",
            "연갈색이 됐더라 그래가지고 인제 연갈색을 또 한 달이 지났어 그리고 또 이 센치가 또 질었겠지.\t연갈색이 됐더라 그래가지고 인제 연갈색을 또 한 달이 지났어 그리고 또 이 센치가 또 질었겠지.\t6\n",
            "그러면 이제 또 또 또 뿌염 해야돼 그러니까 그러면 저는 아니 저 뿌염 안 할게요.\t그러면 이제 또 또 또 뿌염 해야돼 그러니까 그러면 저는 아니 저 뿌염 안 할게요.\t6\n",
            "그냥 (()) 올 염색 해주세요 그래서 올 염색을 했어.\t그냥 (()) 올 염색 해주세요 그래서 올 염색을 했어.\t3\n",
            "그러다 보니까 이제 쪼끔 노란빛이 나더라고 그래가지구 아 점점 애쉬 그레이 색깔로 가고 있구나 했어.\t그러다 보니까 이제 쪼끔 노란빛이 나더라고 그래가지구 아 점점 애쉬 그레이 색깔로 가고 있구나 했어.\t4\n",
            "그러고 났는데 이제 날씨가 점점 추워져 점점 추워지고 노랑머리를 못하겠는 거야.\t그러고 났는데 이제 날씨가 점점 추워져 점점 추워지고 노랑머리를 못하겠는 거야.\t3\n",
            "그래가지고 이제 내가 머리가 쪼끔 노래졌어.\t그래가지고 인자 내가 머리가 쪼끔 노래졌어.\t3\n",
            "근데 나는 그런 색깔이 나는 햇빛에 비쳐진엔 머리 색깔도 모르잖아.\t근데 나는 그런 색깔이 나는 햇빛에 비쳐진엔 머리 색깔도 모르잖아.\t4\n",
            "그래서 나는 그냥 응 쪼끔 노랗구나 하고 갔는데 갑자기 엄마가\t그래서 나는 그냥 응 쪼끔 노랗구나 하고 갔는데 갑자기 엄마가\t4\n",
            "딸\t딸\t5\n",
            "머리 또 염색했어 이렇게 하는 거야 그래서 내가 아니 엄마가 염색 안 했는데 왜 그렇게 하니까 엄마가 나한테\t머리 또 염색했어 이렇게 하는 거야 그래서 내가 아니 엄마가 염색 안 했는데 왜 그렇게 하니까 엄마가 나한테\t3\n",
            "햇빛 받으니까 너 머리지\t햇빛 받으니까 너 머리지\t4\n",
            "짜 노랗다 이렇게 하는 거야. 그래서\t짜 노랗다 이렇게 하는 거야. 그래서\t2\n",
            "내가 노래 뭐 노랗다고 이렇게 하니까\t내가 노래 뭐 노랗다고 이렇게 하니까\t3\n",
            "어 너 햇빛 받으니까 완전 레몬색이다 레몬색이야 이렇게 하는 거야 그래가지구\t어 너 햇빛 받으니까 완전 레몬색이다 레몬색이야 이렇게 하는 거야 그래가지구\t4\n",
            "아니야 엄마 나 아직 노랑 되려면 아직 한참 멀었어 나 이거 엄청 많이 빼야 돼 나 이거 염색 세번인가 밖에 안 했어.\t아니야 엄마 나 아직 노랑 되려면 아직 한참 멀었어 나 이거 엄청 많이 빼야 돼 나 이거 염색 세번인가 밖에 안 했어.\t0\n",
            "원래 탈색해야 되는데 그냥 염색으로 나는 뺀 거야 그래서 머리 갈색이야 이렇게 하니까\t원래 탈색해야 되는데 그냥 염색으로 나는 뺀 거야 그래서 머리 갈색이야 이렇게 하니까\t3\n",
            "아니라는 거야 그래가지구 아 그래 이렇게 하고 이제\t아니라는 거야 그래가지구 아 그래 이렇게 하고 이제\t3\n",
            "또 미용실으로 갔어 또 미용실에 갔더니\t또 미용실으로 갔어 또 미용실에 갔더니\t2\n",
            "이제 다른 색깔로 뭐 뭔 색깔을 하시던 머리 색깔이 나오시겠데 나한테 그래서\t이제 따른 색깔로 뭐 뭔 색깔을 하시던 머리 색깔이 나오시겠데 나한테 그래서\t6\n",
            "아 그래요 원장님 이렇게 하고 그러면 애쉬그레이 해주세요 했더니\t아 그래요 원장님 이렇게 하고 그러면 애쉬그레이 해주세요 했더니\t3\n",
            "요건 안 나와요 또 이렇게 하는 거야 모든 색깔 나온다 해놓고 그래가지구\t요건 안 나와요 또 이렇게 하는 거야 모든 색깔 나온다 해놓고 그래가지구\t2\n",
            "아 그래요 그럼 어떻게 해야될까요 하니까\t아 그래요 그럼 어떻게 해야될까요 하니까\t0\n",
            "이번에는 다른 색깔을 한번 하시고 또 날 좋으면 노란 색깔 또 몇번 했다가 다시 또 이렇게 하시게요 이렇게 하는 거야. 그래서\t이번에는 다른 색깔을 한번 하시고 또 날 좋으면 노란 색깔 또 몇번 했다가 다시 또 이렇게 하시게요 이렇게 하는 거야. 그래서\t2\n",
            "아 예 알겠어요 이렇게 하고 있다가 무슨 색깔 (()) 엄청 고민을 했어 근데 나는 이제 겨울이고 하니까 빨간색깔 하고 싶었어.\t아 예 알겠어요 이렇게 하고 있다가 무슨 색깔 (()) 엄청 고민을 했어 근데 나는 이제 겨울이고 하니까 빨간색깔 하고 싶었어.\t6\n",
            "근데 나랑 엄청 친한 언니가 있는데 그 언니가 머리가 빨간색인 거지.\t근데 나랑 엄청 친한 언니가 있는데 그 언니가 머리가 빨간색인 거지.\t0\n",
            "그래서 나는 이제 똑같은 색깔은 하기 싫은 거야 그래서 내가 찾아본 결과\t그래서 나는 이제 똑같은 색깔은 하기 싫은 거야 그래서 내가 찾아본 결과\t4\n",
            "애쉬 핑크\t애쉬 핑크\t5\n",
            "핑크 브라운 이야 이거는\t핑크 브라운 이야 이거는\t4\n",
            "이거는 핑크 브라운이야 핑크 브라운 핑크 브라운 해달라고 해서 핑크브라운으로 했어.\t이거는 핑크 브라운이야 핑크 브라운 핑크 브라운 해달라고 해서 핑크브라운으로 했어.\t4\n",
            "근데 내 머리가 진짜 개똥 같은 게 색깔이 잘 안 나와 그래가지구\t근데 내 머리가 진짜 개똥 같은 게 색깔이 잘 안 나와 그래가지구\t1\n",
            "핑크 브라운으로 했는데 브라운만 나온 거지 노랑 머리에서 노랑머리\t핑크 브라운으로 했는데 브라운만 나온 거지 노랑 머리에서 노랑머리\t1\n",
            "노랑머리를 내가 뺀 이유가 없는 거야 이 핑크를 빼야 되는데 이게 안 나오니까\t노랑머리를 내가 뺀 이유가 없는 거야 이 핑크를 빼야 되는데 이게 안 나오니까\t6\n",
            "그래서 아 원장님이 이거 잘못된 것 같다 내가 핑크 브라운 해주라 했는데\t그래서 아 원장님이 이거 잘못된 것 같다 내가 핑크 브라운 해주라 했는데\t0\n",
            "했는데 왜 브라운만 나왔냐 이렇게 했어 그랬더니 원장이 나한테 그러는 거야.\t했는데 왜 브라운만 나왔냐 이렇게 했어 그랬더니 원장이 나한테 그러는 거야.\t2\n",
            "아 애쉬 브라운이랑 이렇게 섞었더니 머리가 안나왔네요.\t아 애쉬 브라운이랑 이렇게 섞었더니 머리가 안나왔네요.\t1\n",
            "다시 핑크로 한번 해드릴게요 이렇게 한 거야.\t다시 핑크로 한번 해드릴게요 이렇게 한 거야.\t5\n",
            "그래서 핑크로 했어 근데 이놈의 핑크가 또 안 먹는 거야 그래가지구\t그래서 핑크로 했어 근데 이놈의 핑크가 또 안 먹는 거야 그래가지구\t4\n",
            "어느정도 쪼끔 핑크가 나왔어 그래서 이제 한 이 주 있다가 머리가 또 일 센치 자랐잖아.\t어느정도 쪼끔 핑크가 나왔어 그래서 이제 한 이 주 있다가 머리가 또 일 센치 자랐잖아.\t4\n",
            "이제 뿌염을 할 생각을 하고 다시 갔지 그랬더니 그리고 뿌염도 하고 그다음에 그것을 코팅\t이제 뿌염을 할 생각을 하고 다시 갔지 그랬더니 그리고 뿌염도 하고 그다음에 그것을 코팅\t4\n",
            "코팅을 하러 갔어 근데 빨강머리는 코팅을 하면 더 빨개지고 더 예뻐져.\t코팅을 하러 갔어 근데 빨강머리는 코팅을 하면 더 빨개지고 더 예뻐져.\t5\n",
            "근데 내 같 나 같은 스타일의 머리는 코팅을 해 봤자.\t근데 내 같 나 같은 스타일의 머리는 코팅을 해 봤자.\t4\n",
            "색깔이 안 나온다는 거야 더 촌스러워진대 지금 색깔보다 그래서\t색깔이 안 나온다는 거야 더 촌스러워진대 지금 색깔보다 그래서\t4\n",
            "아 그러면서 원장님 어떻게요 지금 이거 핑크도 아니고 브라운도 아니고 이상한 색깔인데요.\t아 그러면서 원장님 어떻게요 지금 이거 핑크도 아니고 브라운도 아니고 이상한 색깔인데요.\t0\n",
            "이렇게 했어 그랬더니 그러면 원장 원장님이\t이렇게 했어 그랬더니 그러면 원장 원장님이\t1\n",
            "그러면 내가 뿌염하는 금액으로 염색을 한 번 더 해 주겠다 이렇게 하신 거야.\t그러면 내가 뿌염하는 금액으로 염색을 한 번 더 해 주겠다 이렇게 하신 거야.\t4\n",
            "그래서 내가 뿌염하고 핑크 색깔 한 번 더 입혀서 지금 내 머리 색깔이 됐지.\t그래서 내가 뿌염하고 핑크 색깔 한 번 더 입혀서 지금 내 머리 색깔이 됐지.\t4\n",
            "언니 머리 색깔 지금으로 봤을 때는 그런\t언니 머리 색깔 지금으로 봤을 때는 그런\t4\n",
            "고난과 역경을 겪어온 사람 같진 않어 그냥 핑크색인 거 같애.\t고난과 역경을 겪어온 사람 같진 않어 그냥 핑크색인 거 같애.\t4\n",
            "아니 근데 솔직히 봐 봐 내 머리 봐봐 지금 내 핑크가\t아니 근데 솔직히 봐 봐 내 머리 봐봐 지금 내 핑크가\t5\n",
            "지금 여기 막 아 반을 오대오 갈라졌잖아 이 부분만 핑크지 이 안에는 그렇게 핑크 아니야.\t지금 여기 막 아 반을 오대오 갈라졌잖아 이 부분만 핑크지 이 안에는 그렇게 핑크 아니야.\t1\n",
            "어 그리고 아래도 핑크 아니야 언니가 핑크가 비치는 부분은 요 쪽이지.\t어 글고 아래도 핑크 아니야 언니가 핑크가 비치는 부분은 요 쪽이지.\t4\n",
            "여기는 또 노란색이고 여기도 노랑색이야 여기만 핑크야.\t여기는 또 노란색이고 여기도 노랑색이야 여기만 핑크야.\t4\n",
            "그렇지 그러니까 나는 이 핑크 브라운이 자체가 지금 잘못된 거야 왜냐하면 여기 뿌리 뿌리가 나왔어 그 검정이잖아.\t그지 그러니까 나는 이 핑크 브라운이 자체가 지금 잘못된 거야 왜냐하면 여기 뿌리 뿌리가 나왔어 그 껌정이잖아.\t6\n",
            "검정 이 앞에는 찐한 갈색이야.\t껌정 이 앞에는 찐한 갈색이야.\t4\n",
            "진한 갈색이고 여기만 핑크 아래쪽 여기 아래쪽 귀퉁이 옆에\t진한 갈색이고 여기만 핑크 아래쪽 여기 아래쪽 귀퉁이 옆에\t4\n",
            "이 쪽만 핑크고 그다음에 이 귀떼기를 넘어서 이제 턱 쪽으로 내려오는 이 머리 이 여 턱부터\t이 쪽만 핑크고 그다음에 이 귀떼기를 넘어서 이제 턱 쪽으로 내려오는 이 머리 이 여 턱부터\t4\n",
            "여기 인제 내 가슴 정도까지 이 머리는 그냥 갈색인 거야. 그리고\t여기 인제 내 가슴 정도까지 이 머리는 그냥 갈색인 거야. 그리고\t1\n",
            "이 안쪽을 봐 여기 바깥에 말고 이 안쪽을 보면 이 안쪽은 또 다\t이 안쪽을 봐 여기 바깥에 말고 이 안쪽을 보면 이 안쪽은 또 다\t0\n",
            "브라운 색깔이야 그래서 내가 엊그저께 언니집에 갔는데 언니의 아들 둘째 둘째 아들이 나한테 그랬어.\t브라운 색깔이야 그래서 내가 엊그저께 언니집에 갔는데 언니의 아들 둘째 둘째 아들이 나한테 그랬어.\t0\n",
            "이모 이모 염색 다시 해야겠네 이렇게 하는 거야. 그래서\t이모 이모 염색 다시 해야겠네 이렇게 하는 거야. 그래서\t3\n",
            "&name1&아 이모 왜 이모 머리 지금 핑크색 이잖아 왜 다시 해 그러니까 이모\t&name1&아 이모 왜 이모 머리 지금 핑크색 이잖아 왜 다시 해 그러니까 이모\t0\n",
            "여기만 핑크고 다른 데는 다 갈색이잖아.\t여기만 핑크고 다른 데는 다 갈색이잖아.\t4\n",
            "이러면 핑크 머리가 아니지 이렇게 하는 거야 그래가지구\t이러면 핑크 머리가 아니지 이렇게 하는 거야 그래가지구\t1\n",
            "또 아차 싶은 거지 그래서 나 다음달에 또 염색하러 갈 거야.\t또 아차 싶은 거지 그래서 나 다음달에 또 염색하러 갈 거야.\t3\n",
            "그래 염색하러 가서 머리도 좀 올 곧게 썰어봐.\t그래 염색하러 가서 머리도 좀 올 곧게 썰어봐.\t4\n",
            "아니 나 지금은 머리 길고 있는데 뭘 썰으래. 나 지금\t아니 나 지금은 머리 길고 있는데 뭘 썰으래. 나 지금\t4\n",
            "절대로 머리 안 자를 꺼야.\t절대로 머리 안 자를 꺼야.\t2\n",
            "아니 지금 찌찌가 넘었는데 더 긴다고?\t아니 지금 찌찌가 넘었는데 더 긴다고?\t1\n",
            "나 원래 옛날에 머리 찌찌 넘어서 여기 배 배통이 있는 데까지 왔었어 무슨 소리 하는 거야 지금?\t나 원래 옛날에 머리 찌찌 넘어서 여기 배 배통이 있는 데까지 왔었어 무슨 소리 하는 거야 지금?\t2\n",
            "아니 그래도 거기까지 기른다고?\t아니 그래도 거기까지 기른다고?\t4\n",
            "어 나 다시 머리 생머리로 쫙 길꺼야.\t어 나 다시 머리 생머리로 쫙 길꺼야.\t6\n",
            "왜 갑자기?\t왜 갑자기?\t1\n",
            "아니 왜 갑자기 냐니 나 원래 머리 길었었잖아 몰라 기억 안 나니?\t아니 왜 갑자기 냐니 나 원래 머리 길었었잖아 몰라 기억 안 나니?\t1\n",
            "아 지금도 충분히 길어.\t아 지금도 충분히 길어.\t4\n",
            "아니 봐봐 생각해 봐 내가 머리가 옛날에는 배통시까지 왔었잖아 근데 내가 단발로 이렇게 쳤었어 기억하지?\t아니 봐봐 생각해 봐 내가 머리가 옛날에는 배통시까지 왔었잖아 근데 내가 단발로 이렇게 쳤었어 기억하지?\t4\n",
            "언니 배통시까지 긴 사람은 세상에 없어 지금 내 머리 길어 안 길어?\t언니 배통시까지 긴 사람은 세상에 없어 지금 내 머리 길어 안 길어?\t3\n",
            "지금 내 머리 가슴 위야 지금 언니는 여기까지 있어.\t지금 내 머리 가슴 위야 지금 언니는 여기까지 있어.\t3\n",
            "그런데 여기서 여기까지 기른다고 생각해봐 내 머리가 여기까지 내려온다고\t그런데 여기서 여기까지 기른다고 생각해봐 내 머리가 여기까지 내려온다고\t4\n",
            "그게 사람이니 귀신이지 세상에 그렇게 머리 긴 사람은 없어\t그게 사람이니 귀신이지 세상에 그렇게 머리 긴 사람은 없어\t6\n",
            "하여튼 나는 머리를 지금보다 쪼끔 더 길 거야.\t하여튼 나는 머리를 지금보다 쪼끔 더 길 거야.\t4\n",
            "진짜 (()) 싫겠다.\t진짜 (()) 싫겠다.\t3\n",
            "머리 자르고 싶은 생각은 없는데 왜 굳이 머리를 자르라고 그래\t머리 짜르고 싶은 생각은 없는데 왜 굳이 머리를 짜르라고 그래\t2\n",
            "머리 길이도 무조건 길다고 이쁜게 아니라 이쁘게 길르는 거고 이쁜 길이도 정해져있는거야.\t머리 길이도 무조건 길다고 이쁜게 아니라 이쁘게 길르는 거고 이쁜 길이도 정해져있는거야.\t4\n",
            "그래 사람들이 나한테 그랬어 나 단발이 잘 어울린대.\t그래 사람들이 나한테 그랬어 나 단발이 잘 어울린대.\t4\n",
            "저기요 저 단발 잘 어울린다고요.\t저기요 저 단발 잘 어울린다고요.\t5\n",
            "그거는 모르겠는데 일단 길이를\t그거는 모르겠는데 일단 길이를\t4\n",
            "그렇게 배꼽까지 기를 생각은 하지 마 진짜 보기 싫어.\t그렇게 배꼽까지 기를 생각은 하지 마 진짜 비기 싫어.\t6\n",
            "그렇게 기르잖아 아무것도 안 한 채로 그렇게 기르기만 하면 세상에 이런 일이 내보낼 거야.\t그렇게 기르잖아 아무것도 안 한 채로 그렇게 기르기만 하면 세상에 이런 일이 내보낼 거야.\t4\n",
            "세상에 이런 일이 (()) 저한테 그럼 티비 타요?\t세상에 이런 일이 (()) 저한테 그럼 티비 타요?\t1\n",
            "그렇지 머리 안 머리 안 자르는 여자로\t글지 머리 안 머리 안 짜르는 여자로\t2\n",
            "아 그럼 너무 좋죠 그럼 핑크 머리에\t아 그럼 너무 좋죠 그럼 핑크 머리에\t5\n",
            "핑크 머리에 -배- 배꼽까지 오는 머리를 괜찮죠.\t핑크 머리에 -배- 배꼽까지 오는 머리를 괜찮죠.\t5\n",
            "근데 세상에 이런 일이 (()) 나가려면 발가락까지 길어야 되긴 해.\t근데 세상에 이런 일이 (()) 나가려면 발가락까지 길어야 되긴 해.\t1\n",
            "그러니까 발가락 발가락까지 길어야 되는데 그런 사람이 있던데\t그니까 발가락 발가락까지 길어야 되는데 그런 사람이 있던데\t3\n",
            "있지 그니까 미친 놈이 미친 여자 소리 들으면서 세상에 이런 일이 나오는 거야.\t있지 그니까 미친 놈이 미친 여자 소리 들으면서 세상에 이런 일이 나오는 거야.\t2\n",
            "하여튼 그래가지고 나는 요새 지금 머리랑\t하여튼 그래가지고 나는 요새 지금 머리랑\t0\n",
            "머리랑 그다음에 아 머리 난 지금 머리에 가장 중점을 많이 두는 거 같애.\t머리랑 그다음에 아 머리 난 지금 머리에 가장 중점을 많이 두는 거 같애.\t4\n",
            "너는 어떠니?\t너는 어떠니?\t0\n",
            "근데 언니도 알다시피 내가 막 그렇게 그렇게 막 아 근데 원래 관심이 좀 많긴 하지만\t근데 언니도 알다시피 내가 막 그렇게 그렇게 막 아 근데 원래 관심이 좀 많긴 하지만\t3\n",
            "나는 메이크업이나 이런 데에 관심이 많지 막 머리나\t나는 메이크업이나 이런 데에 관심이 많지 막 머리나\t3\n",
            "옷 스타일 막 이런 데 관심이 많진 않잖아 패션인 패션에는 관심이 많이 없잖아 내가\t옷 스타일 막 이런 데 관심이 많진 않잖아 패션인 패션에는 관심이 많이 없잖아 내가\t3\n",
            "그러니까 나는\t긍까 나는\t4\n",
            "예쁘면 된다고 생각하는 사람이거든\t예쁘면 된다고 생각하는 사람이거든\t4\n",
            "당연히 옷도 잘 입으면 좋지 근데 나는 옷을 잘 입는 건 얼굴이라고 생각해.\t당연히 옷도 잘 입으면 좋지 근데 나는 옷을 잘 입는 건 얼굴이라고 생각해.\t4\n",
            "얼굴이 예쁘면 다 잘 어울리고 얼굴이 못생기면은\t얼굴이 예쁘면 다 잘 어울리고 얼굴이 못생기면은\t4\n",
            "그 중에서도 또 잘 어울리는 잘 어울리는 옷이 있겠지 약간 이렇게 생각하는 사람이라서\t그 중에서도 또 잘 어울리는 잘 어울리는 옷이 있겠지 약간 이렇게 생각하는 사람이라서\t3\n",
            "아니 그런데 사실은 이건 핑계고 그냥 옷에 관심이 없어 언니도 옷에 관심없지?\t아니 그런데 사실은 이건 핑계고 그냥 옷에 관심이 없어 언니도 옷에 관심없지?\t3\n",
            "어 나도 옷에 관심없어 난 그냥 대충 옷\t어 나도 옷에 관심없어 난 그냥 대충 옷\t6\n",
            "옷 원피스\t옷 원피스\t4\n",
            "두 개(문은)/(면은) 어떻게 살 거 같애.\t두 개(문은)/(면은) 어떻게 살 거 같애.\t4\n",
            "나는 나는 진짜로 옷에 관심이 없거든 사실 청바지가 다 똑같은 청바지라고 생각하고\t나는 나는 진짜로 옷에 관심이 없거든 사실 청바지가 다 똑같은 청바지라고 생각하고\t3\n",
            "니트가 다 똑같은 니트라고 생각하고\t니트가 다 똑같은 니트라고 생각하고\t4\n",
            "그냥 블라우스도 다 똑같은 블라우스지 뭐 약간 이런 느낌이야 그냥 있으면 입고 없으면 없는 약간 이런 느낌인데\t그냥 블라우스도 다 똑같은 블라우스지 뭐 약간 이런 느낌이야 그냥 있으면 입고 없으면 없는 약간 이런 느낌인데\t4\n",
            "관심이 많은 친구들은 진짜 관심이 많아가지고 이것저것 모으고 막\t관심이 많은 친구들은 진짜 관심이 많아가지고 이것저것 모으고 막\t0\n",
            "이것저것 막 사 모으고 옷 못 버리고 이런 친구들 진짜 많더라고 그리고 나는\t이것저것 막 사 모으고 옷 못 버리고 이런 친구들 진짜 많더라고 그리고 나는\t3\n",
            "메이크업에 관심이 진짜 많으니까\t메이크업에 관심이 진짜 많으니까\t4\n",
            "그러니까 나는 메이크업에 관심이 많다기보다는\t그니까 나는 메이크업에 관심이 많다기보다는\t4\n",
            "내가 할 수 있는 노력을 그냥 메이크업에다가 하는 거 같애 그러니까\t내가 할 수 있는 노력을 그냥 메이크업에다가 하는 거 같애 그니까\t3\n",
            "뭐 내가 머리해도 머리에도 관심이 없고 막 옷에도 관심이 없고 이러니까\t뭐 내가 머리해도 머리에도 관심이 없고 막 옷에도 관심이 없고 이러니까\t3\n",
            "그나마 관심을 갖는 분야가 메이크업이야.\t그나마 관심을 갖는 분야가 메이크업이야.\t4\n",
            "나는 메이크업을 원래 애기 때부터 화장을 안 했어가지구 나는 메이크업에는 관심이 일도 없는 거 같애.\t나는 메이크업을 원래 애기 때부터 화장을 안 했어가지구 나는 메이크업에는 관심이 일도 없는 거 같애.\t6\n",
            "근데 너 화장하는 거 보면 나도 어느정도 너가 좀 조금 부럽기는 해.\t근데 너 화장하는 거 보면 나도 어느정도 너가 좀 쫌 부럽기는 해.\t3\n",
            "왜냐면 아예 예뻐지잖아.\t왜냐면 아예 예뻐지잖아.\t4\n",
            "그러니까 나도 조금 메이크업에 관심을 두고\t그러니까 나도 쫌 메이크업에 관심을 두고\t4\n",
            "하고싶은데 나는 근데 그런 메이크업 소질은 없는 거 같애.\t하고싶은데 나는 근데 그런 메이크업 소질은 없는 거 같애.\t3\n",
            "예뻐지는 게 아니라 그냥 예쁜 사람을 조금 더 예쁘게 만들 뿐이야 못생긴 호박 같은 애를 예쁘게 만들 순 없어.\t예뻐지는 게 아니라 그냥 예쁜 사람을 조금 더 예쁘게 만들 뿐이야 못생긴 호박 같은 애를 예쁘게 만들 순 없어.\t4\n",
            "그럼 나는 어때 나는 될 수 있어?\t그럼 나는 어때 나는 될 수 있어?\t4\n",
            "가능하지 근데 언니는 진짜 메이크업에 미움도 모르잖아 언니는\t가능하지 근데 언니는 진짜 메이크업에 미움도 모르잖아 언니는\t1\n",
            "그 언니는 본인이 하는 걸 못 해 염색은 솔직히 남이 해주는 거잖아.\t그 언니는 본인이 하는 걸 못 해 염색은 솔직히 남이 해주는 거잖아.\t2\n",
            "근데 화장은 내가 내 손으로 해야 되는 거니까\t근데 화장은 내가 내 손으로 해야 되는 거니까\t3\n",
            "사실 어떻게 보면 어렵지 화장하는것도 그리고 나도 사실 내가 화장을 잘 한다고 생각하진 않거든\t사실 어떻게 보면 어렵지 화장하는것도 글고 나도 사실 내가 화장을 잘 한다고 생각하진 않거든\t3\n",
            "그래서 근데 불구하고 욕심을 좀 있지.\t그래서 근데 불구하고 욕심을 좀 있지.\t4\n",
            "화장 잘해야겠다는 욕심이라기 보다는 좋은 화장품을 쓰고 싶은 욕심\t화장 잘해야겠다는 욕심이라기 보다는 좋은 화장품을 쓰고 싶은 욕심\t3\n",
            "그러니까 나는 근데 아예 그런 욕심도 없어 나 로션이랑 그\t그니까 나는 근데 아예 그런 욕심도 없어 나 로션이랑 그\t3\n",
            "스킨 로션 그냥 인터넷에서 만원짜리 사서 쓰거든\t스킨 로션 그냥 인터넷에서 만원짜리 사서 쓰거든\t4\n",
            "그러니까 그런 거 내가 아는 언니한테 얘기 했더니 나한테 그러면 안 됐는데 나 피부 썩는데\t그니까 그런 거 내가 아는 언니한테 얘기 했더니 나한테 그러면 안 됐는데 나 피부 썩는데\t2\n",
            "그러면서 나한테 비싼 거 쓰고 아이라인도 하고 이렇게 아이 아이 크림도 바르고\t그러면서 나한테 비싼 거 쓰고 아이라인도 하고 이렇게 아이 아이 크림도 바르고\t5\n",
            "막 뭐 수분 크림도 바르고 다 하래 근데 나는 전혀 그런 거에 관심이\t막 뭐 수분 크림도 바르고 다 하래 근데 나는 전혀 그런 거에 관심이\t3\n",
            "없으니까 그냥 스킨만 바르고 끝이거든\t없으니까 그냥 스킨만 바르고 끝이거든\t4\n",
            "그러니까 막 관심 좀 가져서 다 하라고 하는 데 관심이\t그러니까 막 관심 좀 가져서 다 하라고 하는 데 관심이\t3\n",
            "내가 좋아야 가져주지 나는 그런 게 없는데 안 가져 되드라고\t내가 좋아야 가져주지 나는 그런 게 없는데 안 가져 되드라고\t3\n",
            "그리구 메이크업은 사실 기초보다는 막 섀도우\t그리구 메이크업은 사실 기초보다는 막 섀도우\t3\n",
            "색깔 막 백 이십까지 있고 막 블러셔 색깔 막 오백까지 입고 립스틱도 막 종류 엄청 많잖아.\t색깔 막 백 이십까지 있고 막 블러셔 색깔 막 오백까지 입고 립스틱도 막 종류 엄청 많잖아.\t6\n",
            "근데 나는 그런 것도 근데 나두 또 무딘게 그런 거에 관심은 또 많이 없어.\t근데 나는 그런 것도 근데 나두 또 무딘게 그런 거에 관심은 또 많이 없어.\t3\n",
            "내가 화장 진짜 잘하는 애들은\t내가 화장 진짜 잘하는 애들은\t2\n",
            "립스틱만 뭐 몇백 개에 몇십개씩 있고\t립스틱만 뭐 몇백 개에 몇십개씩 있고\t4\n",
            "블러셔도 막 엄청 색깔 많고\t블러셔도 막 엄청 색깔 많고\t4\n",
            "눈 화장 여기 위에 그리는 섀도우도 엄청 많고 이러는데 사실 난 그렇진 않거든 그러니까 나는\t눈 화장 여기 위에 그리는 섀도우도 엄청 많고 이러는데 사실 난 그렇진 않거든 그니까 나는\t4\n",
            "예뻐 보이는 몇 개를 사는 거지.\t예뻐 보이는 몇 개를 사는 거지.\t4\n",
            "하늘 아래 같은 색조 없다 이러면서 모든 걸 모으는 취향은 아니고 사실 안 어울리기도 해 내가 막 그렇게\t하늘 아래 같은 색조 없다 이러면서 모든 걸 모으는 취향은 아니고 사실 안 어울리기도 해 내가 막 그렇게\t6\n",
            "화장을 이렇게 화려하게\t화장을 이렇게 화려하게\t5\n",
            "나도 사실 화려하게 내가 직접 해 본 적은 사실 나도 똥손이라서 내가 할 수 있는 것만 하거든 근데\t나도 사실 화려하게 내가 직접 해 본 적은 사실 나도 똥손이라서 내가 할 수 있는 것만 하거든 근데\t5\n",
            "그래서 내가 언니 가끔 화장 해줄 때도 아이라인 찍게 못 그리는 게 내가 찢게 안 그려서\t그래서 내가 언니 가끔 화장 해줄 때도 아이라인 찍게 못 그리는 게 내가 찢게 안 그려서\t0\n",
            "내가 찍게 안 그리니까 남을 그렇게 못해주는 거야 그리고\t내가 찍게 안 그리니까 남을 그렇게 못해주는 거야 그리고\t6\n",
            "그냥 (()) 해봤자 뭐 피부하고 블러셔 하구 근데\t그냥 (()) 해봤자 뭐 피부하고 블러셔 하구 근데\t0\n",
            "최소한의 준비물로 최대의 효과를 내기 위해서 최선을 많이 다 하는 편이지.\t최소한의 준비물로 최대의 효과를 내기 위해서 최선을 많이 다 하는 편이지.\t5\n",
            "근데 나는 그런 거에 이제 일도 관심이 없잖아 그러니까 나는 그런 거 있어 한번씩 밖에 나갔을 때 막\t근데 나는 그런 거에 이제 일도 관심이 없잖아 그니까 나는 그런 거 있어 한번씩 밖에 나갔을 때 막\t3\n",
            "자유부인하고 놀러가거나 막\t자유부인하고 놀러가거나 막\t4\n",
            "친구들 모임 그런 데 가면은 나만 화장을 너무 못하니까 쪼끔\t친구들 모임 그런 데 가면은 나만 화장을 너무 못하니까 쪼끔\t4\n",
            "그렇기는 해 막 다른 애들은 (()) 진짜 변신 막 하고 와.\t그렇기는 해 막 다른 애들은 (()) 진짜 변신 막 하고 와.\t1\n",
            "근데 나는 그냥 하나 안 하나 똑같은 얼굴로 가니까\t근데 나는 그냥 하나 안 하나 똑같은 얼굴로 가니까\t6\n",
            "애들이 옴매 너 화장했냐 안 했냐 이렇게 물어보고\t애들이 옴매 너 화장했냐 안 했냐 이렇게 물어보고\t4\n",
            "응 나 화장했어 그러면 어디 하고 왔냐\t응 나 화장했어 그러면 어디 하고 왔냐\t3\n",
            "이렇게 물어본단 말이야 그런 거 물어봤을 때 그냥 쪼끔 민망하기도 하고 짜증나기도 해.\t이렇게 물어본단 말이야 그런 거 물어봤을 때 그냥 쪼끔 민망하기도 하고 짜증나기도 해.\t4\n",
            "그럴 땐 당당하게 말해\t그럴 땐 당당하게 말해\t4\n",
            "나는 원래 예뻐서 그런다 왜냐면\t나는 원래 예뻐서 그런다 왜냐면\t2\n",
            "내 친구 중에 &name3&이 있지 &name3&이 그때 우리 결혼식 알바 합격할 뻔 할 때 봤지 그게 화장 한 거거든\t내 친구 중에 &name3&이 있지 &name3&이 그때 우리 결혼식 알바 합격할 뻔 할 때 봤지 그게 화장 한 거거든\t1\n",
            "&name3& 항상 &name3&이는 항상\t&name3& 항상 &name3&이는 항상\t0\n",
            "스무 살 때부터 애들 진짜 똑바로 봐 얼굴에 허연 뻘겋고 하고 다닐 때부터 &name3&이는 항상 그대로였어 그래가지구\t스무 살 때부터 애들 진짜 똑바로 봐 얼굴에 허연 뻘겋고 하고 다닐 때부터 &name3&이는 항상 그대로였어 그래가지구\t6\n",
            "사람들이 &name3&씨 막 이런 출근 같은 거 해도 &name3&씨 얼굴에 화장 좀 하고 다녀요 이렇게 하면은\t사람들이 &name3&씨 막 이런 출근 같은 거 해도 &name3&씨 얼굴에 화장 좀 하고 다녀요 이렇게 하면은\t0\n",
            "선생님 저 한 건데요 이렇게 하고 끝내 그리고 다른 사람들이 막 이르케 해도 아이 근데 나는 지금도 충분히 많이 했다고 생각한다.\t선생님 저 한 건데요 이르케 하고 끝내 그리고 다른 사람들이 막 이르케 해도 아이 근데 나는 지금도 충분히 많이 했다고 생각한다.\t3\n",
            "이게 나라고 이게 이 정도가 나라고 생각한다 더하면 답답하고 싫다 라고 말하면\t이게 나라고 이게 이 정도가 나라고 생각한다 더하면 답답하고 싫다 라고 말하면\t6\n",
            "다른 여자들도 본인들도 자기 화장이 답답해 그런데 보이는 눈이 있으니까 어쩔 수 없이 많이 하는 거일 뿐이지\t다른 여자들도 본인들도 자기 화장이 답답해 그런데 보이는 눈이 있으니까 어쩔 수 없이 많이 하는 거일 뿐이지\t3\n",
            "그러니까 그래서 나도 가끔 이제 내가 만약에\t그니까 그래서 나도 가끔 이제 내가 만약에\t0\n",
            "막 그런 애 있잖아 막 무도회장 갈 때 그때는 이제 너한테 해주라고 하잖아.\t막 그런 애 있잖아 막 무도회장 갈 때 그때는 이제 너한테 해주라고 하잖아.\t6\n",
            "그리고 이제 오랜만에 만나는 친구들 만나도 이제 너한테 해달라고 하는데 오늘이 그날이야.\t그리고 이제 오랜만에 만나는 친구들 만나도 이제 너한테 해달라고 하는데 오늘이 그날이야.\t5\n",
            "이렇게 해 가지고 해 주는 해 달라고 하는데\t이렇게 해 가지고 해 주는 해 달라고 하는데\t0\n",
            "나는 막 화장 어떻게 해야 될지도 모르겠고\t나는 막 화장 어떻게 해야 될지도 모르겠고\t3\n",
            "어떤 거를 써 가면서 해야 될지도 모르겠어 그래서 막\t어떤 거를 써 가면서 해야 될지도 모르겠어 그래서 막\t0\n",
            "아이라인을 막 이렇게 그려주면 엄청 막 엄청 찐하게 그려주고 막 하니까\t아이라인을 막 이렇게 그려주면 엄청 막 엄청 찐하게 그려주고 막 하니까\t1\n",
            "근데 그건 그래 근데 그거는 내가 아까 말했다시피 자기 얼굴에 맞는\t근데 그건 그래 근데 그거는 내가 아까 말했다시피 자기 얼굴에 맞는\t4\n",
            "메이크업 방법이 있어 그치 그 &name4& 언니는 그렇게 하는게 어울리는 사람이라서 화장을 그렇게밖에 못 하는 거야.\t메이크업 방법이 있어 그치 그 &name4& 언니는 그렇게 하는게 어울리는 사람이라서 화장을 그렇게밖에 못 하는 거야.\t0\n",
            "그리고 나는 그런 걸 안 하는 사람이라서 난 안 하는\t그리고 나는 그런 걸 안 하는 사람이라서 난 안 하는\t3\n",
            "안 할 수밖에 없는 거고 그러니까\t안 할 수밖에 없는 거고 그니까\t3\n",
            "본인이 우리가 메이크업 아티스트가 아닌 이상\t본인이 우리가 메이크업 아티스트가 아닌 이상\t0\n",
            "모든 사람의 얼굴을 다 해보지는 않잖아 어떤 사람이 이걸 떡 척보면 이게 잘 어울리겠다 척보면 이게 잘 어울리겠다 이런걸 모르잖아.\t모든 사람의 얼굴을 다 해보지는 않잖아 어떤 사람이 이걸 떡 척보면 이게 잘 어울리겠다 척보면 이게 잘 어울리겠다 이런걸 모르잖아.\t4\n",
            "그러니까 그냥 자기가 하는 방법대로 남한테 해주면 안 어울리지 당연히 안 어울리지.\t그러니까 그냥 자기가 하는 방법대로 남한테 해주면 안 어울리지 당연히 안 어울리지.\t6\n",
            "그래서 나도 친구들이 화장해주면 안 어울려.\t그래서 나도 친구들이 화장해주면 안 어울려.\t3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7-z82hXBRnuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "149ba424-db3f-4941-8563-2a8c59cd5c8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>언니 근데 언니 올해 들어서 되게 머리 스타일에 관심이 많아진 거 같애.</td>\n",
              "      <td>언니 근데 언니 올해 들어서 되게 머리 스타일에 관심이 많아진 거 같애.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>어 나 왜 그랬냐면은 올해 우리 아가씨야 우리 아가씨가 그러는 거야.</td>\n",
              "      <td>어 나 왜 그랬냐면은 올해 우리 아가씨야 우리 아가씨가 그러는 거야.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>나한테 언니 언니 머리 너무 까만 색 아니야?</td>\n",
              "      <td>나한테 언니 언니 머리 너무 까만 색 아니야?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>이렇게 하는 거야 그래가지고 내가</td>\n",
              "      <td>이렇게 하는 거야 그래가지고 내가</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>왜요 이렇게 했더니 언니 여름에는 그렇게 까만색 하면 더워보여 이렇게 하는거야.</td>\n",
              "      <td>왜요 이렇게 했더니 언니 여름에는 그렇게 까만색 하면 더워보여 이렇게 하는거야.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>안 할 수밖에 없는 거고 그러니까</td>\n",
              "      <td>안 할 수밖에 없는 거고 그니까</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>본인이 우리가 메이크업 아티스트가 아닌 이상</td>\n",
              "      <td>본인이 우리가 메이크업 아티스트가 아닌 이상</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>모든 사람의 얼굴을 다 해보지는 않잖아 어떤 사람이 이걸 떡 척보면 이게 잘 어울리...</td>\n",
              "      <td>모든 사람의 얼굴을 다 해보지는 않잖아 어떤 사람이 이걸 떡 척보면 이게 잘 어울리...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>그러니까 그냥 자기가 하는 방법대로 남한테 해주면 안 어울리지 당연히 안 어울리지.</td>\n",
              "      <td>그러니까 그냥 자기가 하는 방법대로 남한테 해주면 안 어울리지 당연히 안 어울리지.</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>그래서 나도 친구들이 화장해주면 안 어울려.</td>\n",
              "      <td>그래서 나도 친구들이 화장해주면 안 어울려.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>231 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0  ...  2\n",
              "0             언니 근데 언니 올해 들어서 되게 머리 스타일에 관심이 많아진 거 같애.  ...  3\n",
              "1               어 나 왜 그랬냐면은 올해 우리 아가씨야 우리 아가씨가 그러는 거야.  ...  3\n",
              "2                            나한테 언니 언니 머리 너무 까만 색 아니야?  ...  0\n",
              "3                                   이렇게 하는 거야 그래가지고 내가  ...  3\n",
              "4         왜요 이렇게 했더니 언니 여름에는 그렇게 까만색 하면 더워보여 이렇게 하는거야.  ...  2\n",
              "..                                                 ...  ... ..\n",
              "226                                 안 할 수밖에 없는 거고 그러니까  ...  3\n",
              "227                           본인이 우리가 메이크업 아티스트가 아닌 이상  ...  0\n",
              "228  모든 사람의 얼굴을 다 해보지는 않잖아 어떤 사람이 이걸 떡 척보면 이게 잘 어울리...  ...  4\n",
              "229     그러니까 그냥 자기가 하는 방법대로 남한테 해주면 안 어울리지 당연히 안 어울리지.  ...  6\n",
              "230                           그래서 나도 친구들이 화장해주면 안 어울려.  ...  3\n",
              "\n",
              "[231 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "chatbot_data = pd.read_csv('%s/%s' % (TRAIN_DIR, train_json_list[0]), sep='\\t', header=None)\n",
        "\n",
        "chatbot_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GenDataset(DIR, json_list):\n",
        "  retval = []\n",
        "\n",
        "  for fn in json_list:\n",
        "    #print('%s\\n' % (fn))\n",
        "    fd = pd.read_csv('%s/%s' % (DIR, fn), sep='\\t', header=None)\n",
        "    # [1]데이터가 사투리, [2]가 감정\n",
        "    for q, label in zip(fd[1], fd[2])  :\n",
        "      #print('%s, %s\\n' % (q, label))\n",
        "      data = []\n",
        "      data.append(q)\n",
        "      data.append(str(label))\n",
        "      retval.append(data)\n",
        "\n",
        "  return retval"
      ],
      "metadata": {
        "id": "XVuk1OSmCpbT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = GenDataset(TRAIN_DIR, train_json_list)"
      ],
      "metadata": {
        "id": "4h9HU7A1EJxh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_valid = GenDataset(VALID_DIR, valid_json_list)"
      ],
      "metadata": {
        "id": "DTNZYYXPFDH1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7XIvV17BwAo",
        "outputId": "b9971f71-12ad-4df9-eff8-6c41837c168e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57301\n",
            "26495\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUtyPARZBxUm"
      },
      "source": [
        "#KoBERT 입력 데이터로 변환#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "56vNLMjoB0K7"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W_Cn--xmB2Kg"
      },
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64 # origin : 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nijyNlanB4Ds",
        "outputId": "069f3cae-4576-4f8f-e77c-9203f2c2c7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model\n"
          ]
        }
      ],
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_valid = BERTDataset(dataset_valid, 0, 1, tok, max_len, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxV97d3hB80u",
        "outputId": "a6eeab01-2519-450c-fdb5-25d0c45fbb69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   2, 3241, 5770, 1221, 5850, 3241, 5770, 3450, 1809, 1763, 5400,\n",
              "        2008, 2939, 6896, 1091, 1952, 7344,  862,  830, 6834,  517,   54,\n",
              "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
              " array(23, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       dtype=int32),\n",
              " 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# 첫번째는 패딩된 시퀀스, 두번째는 길이와 티입, 세번째는 어텐션 마스크 시퀀스\n",
        "data_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVg2uv42CE8U",
        "outputId": "848137b6-2e2e-4386-8041-2c103d0ab66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, num_workers=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKyot_LGCHHx"
      },
      "source": [
        "#KoBERT 학습모델 만들기#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LSZ_QCKLCKnG"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zmHD9DjwCMn_"
      },
      "outputs": [],
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/KAIST/SEP531/KoBERT_emotion_finetuned_data_added.pt'), strict=False)\n",
        "model.eval()\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ML2sTd9CN0d"
      },
      "source": [
        "#KoBERT 모델 학습시키기#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "24ea3fcf7f5b425cbeb46c2ad7df8bfc",
            "2c6e6db7cb524de69c01684d40e018f2",
            "025140c592654c8aac4a6565a9665d9e",
            "aba0aafdb48d42bfae06a1d4e5a9bb4e",
            "1c4ba13e987f4ad98f52e5e318aaecc4",
            "b4fe9a3145e44b8b9fa90c1a5b9f813d",
            "8b58bbb869ec4f8dbf4ac1e58ed5261f",
            "d89665d395f54ab3b9f8445bf299ca8c",
            "795deffcb841441ca671cace34bac9c2",
            "e32d4d719e8f45a5bfa6a592f3827927",
            "5b3881b5b5384dcea06e667c8d4ce4e4"
          ]
        },
        "id": "-DP_gmAZCRMf",
        "outputId": "dd88f3d8-a8f2-4039-b05f-266f9b2d7ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24ea3fcf7f5b425cbeb46c2ad7df8bfc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/896 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 batch id 1 loss 0.6892940998077393 train acc 0.71875\n"
          ]
        }
      ],
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO97BB2XHS6a"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), SAVEPOINT_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SEP531/term/4_KoBERT_emotion_finetune_jr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7CVk3qeN9kC1RwOhd2Ano",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24ea3fcf7f5b425cbeb46c2ad7df8bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2c6e6db7cb524de69c01684d40e018f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_025140c592654c8aac4a6565a9665d9e",
              "IPY_MODEL_aba0aafdb48d42bfae06a1d4e5a9bb4e",
              "IPY_MODEL_1c4ba13e987f4ad98f52e5e318aaecc4"
            ]
          }
        },
        "2c6e6db7cb524de69c01684d40e018f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "025140c592654c8aac4a6565a9665d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4fe9a3145e44b8b9fa90c1a5b9f813d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 22%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b58bbb869ec4f8dbf4ac1e58ed5261f"
          }
        },
        "aba0aafdb48d42bfae06a1d4e5a9bb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d89665d395f54ab3b9f8445bf299ca8c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 896,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 198,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_795deffcb841441ca671cace34bac9c2"
          }
        },
        "1c4ba13e987f4ad98f52e5e318aaecc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e32d4d719e8f45a5bfa6a592f3827927",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 198/896 [01:16&lt;04:26,  2.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b3881b5b5384dcea06e667c8d4ce4e4"
          }
        },
        "b4fe9a3145e44b8b9fa90c1a5b9f813d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b58bbb869ec4f8dbf4ac1e58ed5261f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d89665d395f54ab3b9f8445bf299ca8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "795deffcb841441ca671cace34bac9c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e32d4d719e8f45a5bfa6a592f3827927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b3881b5b5384dcea06e667c8d4ce4e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}