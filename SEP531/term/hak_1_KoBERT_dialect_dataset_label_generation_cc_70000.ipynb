{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_KoBERT_dialect_dataset_label_generation_cc_70000.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPswsApvGgqpEiC8sDWzFxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrotherKim/KAIST_CODE/blob/master/SEP531/term/hak_1_KoBERT_dialect_dataset_label_generation_cc_70000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYdNv9u_wZ4",
        "outputId": "6ec43c26-0c32-4a87-9196-6e7153634100"
      },
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FPEfwJNuqA"
      },
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled'\n",
        "TRAIN_DIR = '%s/train' % ROOT_DIR\n",
        "VALID_DIR = '%s/valid' % ROOT_DIR\n",
        "\n",
        "NUM_TRAIN = 171\n",
        "NUM_VALID = 58\n",
        "\n",
        "SAVEPOINT_PATH = '/content/drive/MyDrive/KAIST/SEP531/KoBERT_emotion_70000_std.pt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9TKL9NZ_Dh-"
      },
      "source": [
        "#실행 환경(사용)#\n",
        "\n",
        "- Python >= 3.6\n",
        "- PyTorch >= 1.70\n",
        "- Transformers = 3.0.2\n",
        "- Colab\n",
        "- batch size = 64 (convertable)\n",
        "- epochs = 10 (convertable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU6f6roY-5sW",
        "outputId": "84169a79-f514-4440-93aa-73e83dd47258"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9 MB 1.9 MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.10.8)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.6)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595742 sha256=7b4c00af2c220a020885e03388dc2dfb0d4171bead900e5c35542feb449e7cce\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 11.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlNTmU-_WS9"
      },
      "source": [
        "#KoBERT 다운로드(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12JYVZJ_al4",
        "outputId": "9186b137-bc1c-4ef0-b631-8dc9159c3fd6"
      },
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-u5lp_2q0\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-u5lp_2q0\n",
            "Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.10.0)\n",
            "Requirement already satisfied: mxnet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.8.0.post0)\n",
            "Collecting onnxruntime>=0.3.0\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.1.2) (1.10.0+cu111)\n",
            "Collecting transformers>=4.8.1\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (0.29.24)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (21.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.1.2) (2.23.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (3.17.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.1.2) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.1.2) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 489 kB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 26.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.1.2) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.1.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime>=0.3.0->kobert==0.1.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (1.1.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=13136 sha256=9bec45ab8215d5a3a07d7bc5bcb06a03d33b5caf50a89a2ed7ce6958059b8b74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15hwpa43/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, onnxruntime, kobert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed huggingface-hub-0.2.1 kobert-0.1.2 onnxruntime-1.10.0 pyyaml-6.0 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG56WkzA_ccN"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enK_3xLH_ecb"
      },
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x35BKT7q_gAU"
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoVIALyi_hR_",
        "outputId": "624d82f3-8fbd-4bb8-ea6a-c2dbc3097b76"
      },
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            ".cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUtyPARZBxUm"
      },
      "source": [
        "#KoBERT 입력 데이터로 변환(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56vNLMjoB0K7"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Cn--xmB2Kg"
      },
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAOyPXXC2_x"
      },
      "source": [
        "#원본 데이터셋으로부터 데이터 추출하기#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpuTUhB2Npyo"
      },
      "source": [
        "# 터미널 커맨드를 문자열로 반환해주는 함수 작성\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def GetShellCmdStdOut(command):\n",
        "  cmd = ['sh', '-c', command]\n",
        "  fd_popen = subprocess.Popen(cmd, stdout=subprocess.PIPE).stdout \n",
        "  data = fd_popen.read().strip() \n",
        "  fd_popen.close()\n",
        "\n",
        "  retval = data.decode('utf-8') \n",
        "  return retval"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxMSGeurHAkO",
        "outputId": "618ec9e2-3dba-4998-ae95-40fc5da0f725"
      },
      "source": [
        "!echo $VALID_DIR"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX4CsLP7LvGO",
        "outputId": "d7625b3b-7f29-4466-efe5-540a758c0568"
      },
      "source": [
        "%pushd $TRAIN_DIR\n",
        "tl = GetShellCmdStdOut('find | grep json | grep -v data | grep -v answer| grep -v config').split('\\n')\n",
        "train_json_list = [x.replace('./', '') for x in tl]\n",
        "train_json_list.sort()\n",
        "%popd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_cc_labeled/train\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGRX2OgDkKtC",
        "outputId": "8c069fea-95bf-4ebd-efcf-e778f71df510"
      },
      "source": [
        "%pushd $VALID_DIR\n",
        "vl = GetShellCmdStdOut('find | grep json | grep -v data | grep -v answer | grep -v config').split('\\n')\n",
        "valid_json_list = [x.replace('./', '') for x in vl]\n",
        "valid_json_list.sort()\n",
        "%popd"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_cc_labeled/valid\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y5Il9xoPztk",
        "outputId": "1528725b-fc4c-44e0-d102-786d48ead5dd"
      },
      "source": [
        "len(train_json_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5651"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ax21CMQI9Uw",
        "outputId": "ee5c4570-06c4-44d4-c6b6-25164ea3b10e"
      },
      "source": [
        "len(valid_json_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "780"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH_DSjp7CXW9"
      },
      "source": [
        "#추출한 데이터에 감정 레이블 추가(사용)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZ_QCKLCKnG"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zowlRfd3Cduu"
      },
      "source": [
        "새로운 문장을 테스트 할 때, 입력되는 문장을 KoBERT의 입력 형식으로 바꿔주는 코드를 작성해주어야 한다. 아래 코드를 작성하여 토큰화, 패딩, 텐서를 바꿔주고 예측을 하는 'predict' 함수를 만들어 주었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Mlpc2DHjXb",
        "outputId": "0d65e1e2-a1ce-4919-d42a-d4bb133f0c7a"
      },
      "source": [
        "#모델 불러오기\n",
        "model1 = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "model1.load_state_dict(torch.load(SAVEPOINT_PATH), strict=False)\n",
        "model1.eval()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAROeR5XCW0d",
        "outputId": "4c3f21d0-5314-49ad-ecb7-b4386439b673"
      },
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "dic = {0:'공포', 1:'놀람', 2:'분노', 3:'슬픔', 4:'중립', 5:'행복', 6:'혐오'}\n",
        "def getEmotion(out):\n",
        "  idx_eval=[]\n",
        "  val_eval=[]\n",
        "  for i in out:\n",
        "    logits=i\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    idx = np.argmax(logits)\n",
        "    idx_eval.append(idx)\n",
        "    val_eval.append(dic.get(idx))\n",
        "    return idx_eval[0], val_eval[0]\n",
        "\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model1.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model1(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        return getEmotion(out)\n",
        "\n",
        "\n",
        "        # file wrting"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2rDG1Tane3j"
      },
      "source": [
        "import csv\n",
        "\n",
        "def GenLabelFromData70000(path, jsonfilename):\n",
        "  try:\n",
        "    fin = open('%s/%s.data' % (path, jsonfilename), 'r')\n",
        "  except:\n",
        "    print('No file(%s/%s.data)' % (path, jsonfilename))\n",
        "    return\n",
        "\n",
        "  with open('%s/%s.data' % (path, jsonfilename), 'r') as f:\n",
        "    with open('%s/%s.answer70000' % (path, jsonfilename), 'w') as o:\n",
        "      csv_data = csv.reader(f, delimiter='\\t')\n",
        "      for i, line in enumerate(csv_data):\n",
        "        std = line[0]\n",
        "        dia = line[1]\n",
        "        idx, val = predict(line[0])\n",
        "        #print('%s(%d) %s\\n' % (val, idx, std))\n",
        "        #print('[%d]%s/%s/%d/%s\\n' % (i, std, dia, idx, val))\n",
        "        o.write('%s\\t%s\\t%d\\n' % (std, dia, idx))\n",
        "  print('>> Generated file %s/%s.answer\\n' % (path, jsonfilename))\n",
        "\n",
        "  #with open('%s/%s.data' % (path, jsonfilename), 'w') as f:\n",
        "  #  for u in ut:\n",
        "  #    f.write('%s\\t%s\\n' % (u['standard_form'], u['dialect_form']))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LK5f-dczr3U",
        "outputId": "bbb90f47-5088-4510-9e3c-e83d9a11bdfa"
      },
      "source": [
        "train_json_list[0:171]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DCDG20000001.json',\n",
              " 'DCDG20000002.json',\n",
              " 'DCDG20000003.json',\n",
              " 'DCDG20000004.json',\n",
              " 'DCDG20000005.json',\n",
              " 'DCDG20000006.json',\n",
              " 'DCDG20000007.json',\n",
              " 'DCDG20000008.json',\n",
              " 'DCDG20000403.json',\n",
              " 'DCDG20000404.json',\n",
              " 'DCDG20000405.json',\n",
              " 'DCDG20000406.json',\n",
              " 'DCDG20000407.json',\n",
              " 'DCDG20000408.json',\n",
              " 'DCDG20000409.json',\n",
              " 'DCDG20000410.json',\n",
              " 'DCDG20000411.json',\n",
              " 'DCDG20000412.json',\n",
              " 'DCDG20000413.json',\n",
              " 'DCDG20000414.json',\n",
              " 'DCDG20000415.json',\n",
              " 'DCDG20000416.json',\n",
              " 'DCDG20000417.json',\n",
              " 'DCDG20000418.json',\n",
              " 'DCDG20000419.json',\n",
              " 'DCDG20000420.json',\n",
              " 'DCDG20000421.json',\n",
              " 'DCDG20000422.json',\n",
              " 'DCDG20000423.json',\n",
              " 'DCDG20000424.json',\n",
              " 'DCDG20000425.json',\n",
              " 'DCDG20000426.json',\n",
              " 'DCDG20000427.json',\n",
              " 'DCDG20000428.json',\n",
              " 'DCDG20000429.json',\n",
              " 'DCDG20000430.json',\n",
              " 'DCDG20000431.json',\n",
              " 'DCDG20000432.json',\n",
              " 'DCDG20000433.json',\n",
              " 'DCDG20000434.json',\n",
              " 'DCDG20000435.json',\n",
              " 'DCDG20000436.json',\n",
              " 'DCDG20000437.json',\n",
              " 'DCDG20000438.json',\n",
              " 'DCDG20000439.json',\n",
              " 'DCDG20000440.json',\n",
              " 'DCDG20000441.json',\n",
              " 'DCDG20000442.json',\n",
              " 'DCDG20000443.json',\n",
              " 'DCDG20000444.json',\n",
              " 'DCDG20000445.json',\n",
              " 'DCDG20000446.json',\n",
              " 'DCDG20000447.json',\n",
              " 'DCDG20000448.json',\n",
              " 'DCDG20000449.json',\n",
              " 'DCDG20000450.json',\n",
              " 'DCDG20000451.json',\n",
              " 'DCDG20000452.json',\n",
              " 'DCDG20000453.json',\n",
              " 'DCDG20000454.json',\n",
              " 'DCDG20000455.json',\n",
              " 'DCDG20000456.json',\n",
              " 'DCDG20000457.json',\n",
              " 'DCDG20000458.json',\n",
              " 'DCDG20000459.json',\n",
              " 'DCDG20000460.json',\n",
              " 'DCDG20000461.json',\n",
              " 'DCDG20000462.json',\n",
              " 'DCDG20000463.json',\n",
              " 'DCDG20000464.json',\n",
              " 'DCDG20000465.json',\n",
              " 'DCDG20000466.json',\n",
              " 'DCDG20000470.json',\n",
              " 'DCDG20000471.json',\n",
              " 'DCDG20000472.json',\n",
              " 'DCDG20000473.json',\n",
              " 'DCDG20000474.json',\n",
              " 'DCDG20000475.json',\n",
              " 'DCDG20000476.json',\n",
              " 'DCDG20000477.json',\n",
              " 'DCDG20000478.json',\n",
              " 'DCDG20000479.json',\n",
              " 'DCDG20000480.json',\n",
              " 'DCDG20000481.json',\n",
              " 'DCDG20000482.json',\n",
              " 'DCDG20000483.json',\n",
              " 'DCDG20000484.json',\n",
              " 'DCDG20000485.json',\n",
              " 'DCDG20000486.json',\n",
              " 'DCDG20000487.json',\n",
              " 'DCDG20000488.json',\n",
              " 'DCDG20000489.json',\n",
              " 'DCDG20000490.json',\n",
              " 'DCDG20000491.json',\n",
              " 'DCDG20000492.json',\n",
              " 'DCDG20000493.json',\n",
              " 'DCDG20000494.json',\n",
              " 'DCDG20000495.json',\n",
              " 'DCDG20000496.json',\n",
              " 'DCDG20000497.json',\n",
              " 'DCDG20000498.json',\n",
              " 'DCDG20000499.json',\n",
              " 'DCDG20000500.json',\n",
              " 'DCDG20000501.json',\n",
              " 'DCDG20000502.json',\n",
              " 'DCDG20000504.json',\n",
              " 'DCDG20000505.json',\n",
              " 'DCDG20000506.json',\n",
              " 'DCDG20000507.json',\n",
              " 'DCDG20000508.json',\n",
              " 'DCDG20000509.json',\n",
              " 'DCDG20000510.json',\n",
              " 'DCDG20000511.json',\n",
              " 'DCDG20000512.json',\n",
              " 'DCDG20000513.json',\n",
              " 'DCDG20000514.json',\n",
              " 'DCDG20000515.json',\n",
              " 'DCDG20000516.json',\n",
              " 'DCDG20000517.json',\n",
              " 'DCDG20000518.json',\n",
              " 'DCDG20000519.json',\n",
              " 'DCDG20000520.json',\n",
              " 'DCDG20000521.json',\n",
              " 'DCDG20000522.json',\n",
              " 'DCDG20000523.json',\n",
              " 'DCDG20000524.json',\n",
              " 'DCDG20000525.json',\n",
              " 'DCDG20000526.json',\n",
              " 'DCDG20000527.json',\n",
              " 'DCDG20000528.json',\n",
              " 'DCDG20000529.json',\n",
              " 'DCDG20000530.json',\n",
              " 'DCDG20000531.json',\n",
              " 'DCDG20000532.json',\n",
              " 'DCDG20000533.json',\n",
              " 'DCDG20000534.json',\n",
              " 'DCDG20000535.json',\n",
              " 'DCDG20000536.json',\n",
              " 'DCDG20000537.json',\n",
              " 'DCDG20000538.json',\n",
              " 'DCDG20000539.json',\n",
              " 'DCDG20000540.json',\n",
              " 'DCDG20000541.json',\n",
              " 'DCDG20000542.json',\n",
              " 'DCDG20000543.json',\n",
              " 'DCDG20000544.json',\n",
              " 'DCDG20000545.json',\n",
              " 'DCDG20000546.json',\n",
              " 'DCDG20000547.json',\n",
              " 'DCDG20000548.json',\n",
              " 'DCDG20000549.json',\n",
              " 'DCDG20000550.json',\n",
              " 'DCDG20000551.json',\n",
              " 'DCDG20000555.json',\n",
              " 'DCDG20000556.json',\n",
              " 'DCDG20000557.json',\n",
              " 'DCDG20000558.json',\n",
              " 'DCDG20000559.json',\n",
              " 'DCDG20000560.json',\n",
              " 'DCDG20000561.json',\n",
              " 'DCDG20000562.json',\n",
              " 'DCDG20000563.json',\n",
              " 'DCDG20000564.json',\n",
              " 'DCDG20000565.json',\n",
              " 'DCDG20000566.json',\n",
              " 'DCDG20000567.json',\n",
              " 'DCDG20000568.json',\n",
              " 'DCDG20000569.json',\n",
              " 'DCDG20000570.json',\n",
              " 'DCDG20000571.json',\n",
              " 'DCDG20000572.json']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_json_list[0:58]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iO690ueczGs",
        "outputId": "7deb8629-c5d0-484a-e18c-2edde1042e08"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DCDG20000009.json',\n",
              " 'DCDG20000010.json',\n",
              " 'DCDG20000011.json',\n",
              " 'DCDG20000012.json',\n",
              " 'DCDG20000013.json',\n",
              " 'DCDG20000014.json',\n",
              " 'DCDG20000015.json',\n",
              " 'DCDG20000016.json',\n",
              " 'DCDG20000017.json',\n",
              " 'DCDG20000018.json',\n",
              " 'DCDG20000019.json',\n",
              " 'DCDG20000020.json',\n",
              " 'DCDG20000021.json',\n",
              " 'DCDG20000022.json',\n",
              " 'DCDG20000023.json',\n",
              " 'DCDG20000024.json',\n",
              " 'DCDG20000025.json',\n",
              " 'DCDG20000026.json',\n",
              " 'DCDG20000027.json',\n",
              " 'DCDG20000028.json',\n",
              " 'DCDG20000029.json',\n",
              " 'DCDG20000030.json',\n",
              " 'DCDG20000031.json',\n",
              " 'DCDG20000032.json',\n",
              " 'DCDG20000033.json',\n",
              " 'DCDG20000034.json',\n",
              " 'DCDG20000035.json',\n",
              " 'DCDG20000036.json',\n",
              " 'DCDG20000037.json',\n",
              " 'DCDG20000038.json',\n",
              " 'DCDG20000039.json',\n",
              " 'DCDG20000040.json',\n",
              " 'DCDG20000041.json',\n",
              " 'DCDG20000042.json',\n",
              " 'DCDG20000043.json',\n",
              " 'DCDG20000044.json',\n",
              " 'DCDG20000045.json',\n",
              " 'DCDG20000046.json',\n",
              " 'DCDG20000047.json',\n",
              " 'DCDG20000048.json',\n",
              " 'DCDG20000049.json',\n",
              " 'DCDG20000050.json',\n",
              " 'DCDG20000051.json',\n",
              " 'DCDG20000052.json',\n",
              " 'DCDG20000053.json',\n",
              " 'DCDG20000054.json',\n",
              " 'DCDG20000055.json',\n",
              " 'DCDG20000056.json',\n",
              " 'DCDG20000057.json',\n",
              " 'DCDG20000058.json',\n",
              " 'DCDG20000059.json',\n",
              " 'DCDG20000060.json',\n",
              " 'DCDG20000061.json',\n",
              " 'DCDG20000062.json',\n",
              " 'DCDG20000063.json',\n",
              " 'DCDG20000064.json',\n",
              " 'DCDG20000065.json',\n",
              " 'DCDG20000066.json']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for문을 돌면서 json 파일 내에 있는 표준어 문장과 사투리 문장을 추출해서 FILENAME.data로 저장\n",
        "import json\n",
        "\n",
        "def GenDataFromJson(path, jsonfilename):\n",
        "  with open('%s/%s' % (path, jsonfilename), 'r') as f:\n",
        "    json_data = json.load(f);\n",
        "  #print(json.dumps(json_data, indent='\\t'))\n",
        "  ut = json_data['utterance']\n",
        "\n",
        "  with open('%s/%s.data' % (path, jsonfilename), 'w') as f:\n",
        "    for u in ut:\n",
        "      f.write('%s\\t%s\\n' % (u['standard_form'], u['dialect_form']))\n",
        "   \n",
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 * 350개 문장\n",
        "#for f in train_json_list[0:171]:\n",
        "#  GenDataFromJson(TRAIN_DIR, f)\n",
        "#%popd\n",
        "\n",
        "#%pushd $VALID_DIR\n",
        "# 150개 파일 350개 문장\n",
        "#for f in valid_json_list[0:58]:\n",
        "#  GenDataFromJson(VALID_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjC9V3PACo76",
        "outputId": "64d33b47-8416-4d6b-95a1-e3f825bc6e93"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_cc_labeled/train\n",
            "/content\n",
            "popd -> /content\n",
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_cc_labeled/valid\n",
            "/content\n",
            "popd -> /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNvc_w1DoB0g",
        "outputId": "ac176de7-aa83-465d-d96e-403a42387b51"
      },
      "source": [
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 350개 문장\n",
        "#for i, f in enumerate(train_json_list[0:100]):\n",
        "#  print('[%d][%s]' % (i, f))\n",
        "#  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "#popd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1-0pJ0cGHz95nSQSccZedRVC_hwqx-59q/KAIST/SEP531/kor_cc_labeled/train\n",
            "[0][DCDG20000001.json]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000001.json.answer\n",
            "\n",
            "[1][DCDG20000002.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000002.json.answer\n",
            "\n",
            "[2][DCDG20000003.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000003.json.answer\n",
            "\n",
            "[3][DCDG20000004.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000004.json.answer\n",
            "\n",
            "[4][DCDG20000005.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000005.json.answer\n",
            "\n",
            "[5][DCDG20000006.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000006.json.answer\n",
            "\n",
            "[6][DCDG20000007.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000007.json.answer\n",
            "\n",
            "[7][DCDG20000008.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000008.json.answer\n",
            "\n",
            "[8][DCDG20000403.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000403.json.answer\n",
            "\n",
            "[9][DCDG20000404.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000404.json.answer\n",
            "\n",
            "[10][DCDG20000405.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000405.json.answer\n",
            "\n",
            "[11][DCDG20000406.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000406.json.answer\n",
            "\n",
            "[12][DCDG20000407.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000407.json.answer\n",
            "\n",
            "[13][DCDG20000408.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000408.json.answer\n",
            "\n",
            "[14][DCDG20000409.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000409.json.answer\n",
            "\n",
            "[15][DCDG20000410.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000410.json.answer\n",
            "\n",
            "[16][DCDG20000411.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000411.json.answer\n",
            "\n",
            "[17][DCDG20000412.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000412.json.answer\n",
            "\n",
            "[18][DCDG20000413.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000413.json.answer\n",
            "\n",
            "[19][DCDG20000414.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000414.json.answer\n",
            "\n",
            "[20][DCDG20000415.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000415.json.answer\n",
            "\n",
            "[21][DCDG20000416.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000416.json.answer\n",
            "\n",
            "[22][DCDG20000417.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000417.json.answer\n",
            "\n",
            "[23][DCDG20000418.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000418.json.answer\n",
            "\n",
            "[24][DCDG20000419.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000419.json.answer\n",
            "\n",
            "[25][DCDG20000420.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000420.json.answer\n",
            "\n",
            "[26][DCDG20000421.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000421.json.answer\n",
            "\n",
            "[27][DCDG20000422.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000422.json.answer\n",
            "\n",
            "[28][DCDG20000423.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000423.json.answer\n",
            "\n",
            "[29][DCDG20000424.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000424.json.answer\n",
            "\n",
            "[30][DCDG20000425.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000425.json.answer\n",
            "\n",
            "[31][DCDG20000426.json]\n",
            ">> Generated file /content/drive/MyDrive/KAIST/SEP531/kor_cc_labeled/train/DCDG20000426.json.answer\n",
            "\n",
            "[32][DCDG20000427.json]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(train_json_list[30:171]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "id": "U6sj1yVtmNBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXPq8G1PxaAp"
      },
      "source": [
        "%pushd $TRAIN_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(train_json_list[101:171]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(TRAIN_DIR, f)\n",
        "%popd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pushd $VALID_DIR\n",
        "# 450개 파일 350개 문장\n",
        "for i, f in enumerate(valid_json_list[0:58]):\n",
        "  print('[%d][%s]' % (i, f))\n",
        "  GenLabelFromData70000(VALID_DIR, f)\n",
        "%popd"
      ],
      "metadata": {
        "id": "-U0K1DGn_ehJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}