{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SET531/Project/KoGPT2_chatbot_tutorial.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOwdzxlDX3EANsDZjOIKwr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrotherKim/Colab/blob/main/SEP531/KoGPT2_chatbot_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOqL-ERxwMDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0663f0e6-bdb5-4eec-eae5-26320c126099"
      },
      "source": [
        "# GPU 정보 \n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct  3 10:14:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUbPw_7Bf9Jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d64bdf-01d0-4804-f075-de309ac55c3c"
      },
      "source": [
        "# 현재 CUDA 10.1이라 mxnet-cu101로 설치\n",
        "# CUDA에 맞는 mxnet 버전을 선택해야함 (https://mxnet.apache.org/get_started 참조)\n",
        "# 만약 GPU 사용을 하고 있지 않다는 문구가 뜬다면 올바른 버전이 아님\n",
        "!pip install mxnet-cu101"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-cu101\n",
            "  Downloading mxnet_cu101-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (356.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 356.7 MB 25 kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.8.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WKGlGahhDAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344a6678-9b5b-48a2-87e2-226e0431187b"
      },
      "source": [
        "# 패키지 설치\n",
        "!pip install gluonnlp sentencepiece pandas"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595725 sha256=132bf785b0c8b7570e2a06424032b9a9bcf66625ac4916ffd0e59bf6b4edcfc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: sentencepiece, gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU2QCWaZh_54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cff1629-3588-4146-d15b-27d7642db7f5"
      },
      "source": [
        "# SKT의 KoGPT2 설치\n",
        "!git clone https://github.com/SKT-AI/KoGPT2.git\n",
        "#%cd KoGPT2\n",
        "#!pip install ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoGPT2'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 199 (delta 40), reused 40 (delta 15), pack-reused 121\u001b[K\n",
            "Receiving objects: 100% (199/199), 652.99 KiB | 18.66 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHfMpPhlpbqQ",
        "outputId": "17f9adbb-7373-4148-99d6-c80a82094994"
      },
      "source": [
        "%cd KoGPT2\n",
        "!pip install \n",
        "%cd .."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'KoGPT2'\n",
            "/content/KoGPT2\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8W3gZk2ijYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c17216-6358-42f7-eebe-4a27a539b0ab"
      },
      "source": [
        "# KoGPT2-chatbot 복사\n",
        "!git clone --recurse-submodules https://github.com/haven-jeon/KoGPT2-chatbot.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoGPT2-chatbot'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 100 (delta 6), reused 8 (delta 2), pack-reused 85\u001b[K\n",
            "Receiving objects: 100% (100/100), 115.32 KiB | 6.41 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n",
            "Submodule 'Chatbot_data' (https://github.com/haven-jeon/Chatbot_data.git) registered for path 'Chatbot_data'\n",
            "Cloning into '/content/KoGPT2-chatbot/Chatbot_data'...\n",
            "remote: Enumerating objects: 24, done.        \n",
            "remote: Counting objects: 100% (4/4), done.        \n",
            "remote: Compressing objects: 100% (4/4), done.        \n",
            "remote: Total 24 (delta 0), reused 3 (delta 0), pack-reused 20        \n",
            "Submodule path 'Chatbot_data': checked out '235fac5aea3badab22743f7048afe936cf72f822'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ZweKmXiuaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a64928-404d-49c3-f721-6dcf880937fc"
      },
      "source": [
        "# 폴더 이동\n",
        "%cd KoGPT2-chatbot"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoGPT2-chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJMYV-aXpBt9",
        "outputId": "00afa538-df2e-4329-cc37-38646ef7f713"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot_data  KoGPT2_chatbot_pytorch.ipynb  README.md\t      train_torch.py\n",
            "imgs\t      LICENSE\t\t\t    requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVwAwrOcqO23",
        "outputId": "833c5d67-47a0-4eb2-e238-d496803e3786"
      },
      "source": [
        "%pip install -r requirements.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Collecting pytorch_lightning==1.2.10\n",
            "  Downloading pytorch_lightning-1.2.10-py3-none-any.whl (841 kB)\n",
            "\u001b[K     |████████████████████████████████| 841 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.9.0+cu102)\n",
            "Collecting transformers==4.5.1\n",
            "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 61.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics==0.2.0\n",
            "  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 94.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.62.3)\n",
            "Collecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 84.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 80.0 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 81.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 3)) (3.7.4.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 83.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.40.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 74.6 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning==1.2.10->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 4)) (7.1.2)\n",
            "Building wheels for collected packages: future, PyYAML\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=66c71f6c21397e9eafdde2591167781ed5b838253c285bde7d8cc01f72ce4dda\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44635 sha256=d8f60cc1b18b84aad5b0a64a2912ec48e99f34766720283448b8a14a18e2351e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "Successfully built future PyYAML\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, tokenizers, sacremoses, PyYAML, future, transformers, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.10.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.10 sacremoses-0.0.46 tokenizers-0.10.3 torchmetrics-0.2.0 transformers-4.5.1 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMZv-ZsiqkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0e3a4e-1fe5-4324-a3b6-a888f3be065d"
      },
      "source": [
        "# 사전훈련된 KoGPT2를 챗봇 데이터로 파인튜닝\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --train --max_epochs 2"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=2, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=True, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "INFO:filelock:Lock 139788062979152 acquired on /root/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
            "Downloading: 100% 1.00k/1.00k [00:00<00:00, 1.00MB/s]\n",
            "INFO:filelock:Lock 139788062979152 released on /root/.cache/huggingface/transformers/13bb826cf24517d7849a701e02452715a67c5e560142be3d4735442b2a545809.6b384eec6effdd44287f67715cd55bd0dff2cf846d843b932b43ba7b632b8b1e.lock\n",
            "INFO:filelock:Lock 139788056187344 acquired on /root/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
            "Downloading: 100% 513M/513M [00:07<00:00, 70.7MB/s]\n",
            "INFO:filelock:Lock 139788056187344 released on /root/.cache/huggingface/transformers/495b405e3742953dbcc56685d1560fa02a2d86fc50b891868990a4471b06c934.4ebf112d34c2c8fc657866680005d92d21859c52c0ef5e941fa640129b2f8f88.lock\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | kogpt2        | GPT2LMHeadModel  | 125 M \n",
            "1 | loss_function | CrossEntropyLoss | 0     \n",
            "---------------------------------------------------\n",
            "125 M     Trainable params\n",
            "0         Non-trainable params\n",
            "125 M     Total params\n",
            "500.656   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Epoch 0:   0% 0/124 [00:00<?, ?it/s] INFO:root:contexts : 견딘다는 것\n",
            "INFO:root:contexts : 보고싶어 사무칠때\n",
            "INFO:root:toked ctx: ['<usr>', '▁견딘', '다는', '▁것', '<unused1>', '▁1']\n",
            "INFO:root:toked ctx: ['<usr>', '▁보고', '싶', '어', '▁사무', '칠', '때', '<unused1>', '▁1']\n",
            "INFO:root:response : 오늘을 살아가는 것.\n",
            "INFO:root:response : 다른 일에 집중해보는 건 어떨까요.\n",
            "INFO:root:toked response : ['<sys>', '▁오늘', '을', '▁살아가는', '▁것.', '</s>']\n",
            "INFO:root:toked response : ['<sys>', '▁다른', '▁일에', '▁집중', '해', '보는', '▁건', '▁어', '떨', '까', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁오늘', '을', '▁살아가는', '▁것.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁다른', '▁일에', '▁집중', '해', '보는', '▁건', '▁어', '떨', '까', '요.', '</s>']\n",
            "Epoch 0: 100% 124/124 [01:08<00:00,  1.82it/s, loss=28.1, v_num=0]Epoch 0, global step 123: train_loss reached 30.21531 (best 30.21531), saving model to \"/content/KoGPT2-chatbot/model_chp/model_-epoch=00-train_loss=30.22.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1180934144 bytes == 0x5563adb1c000 @  0x7f23cad18615 0x5562fe3a34cc 0x5562fe48347a 0x5562fe3a9f0c 0x7f239fbd85d4 0x7f239fbe0cd4 0x7f239fbb57e0 0x7f238de79665 0x7f238de74f9e 0x7f238de7a13a 0x7f239fbb5d2e 0x7f239f64bb88 0x5562fe3a7098 0x5562fe41a4d9 0x5562fe414ced 0x5562fe3a7bda 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe415c0d 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415c0d 0x5562fe3a7afa\n",
            "tcmalloc: large alloc 1476173824 bytes == 0x5563f4a58000 @  0x7f23cad18615 0x5562fe3a34cc 0x5562fe48347a 0x5562fe3a9f0c 0x7f239fbd85d4 0x7f239fbe0cd4 0x7f239fbb57e0 0x7f238de79665 0x7f238de74f9e 0x7f238de7a13a 0x7f239fbb5d2e 0x7f239f64bb88 0x5562fe3a7098 0x5562fe41a4d9 0x5562fe414ced 0x5562fe3a7bda 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe415c0d 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415c0d 0x5562fe3a7afa\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x5564500fa000 @  0x7f23cad18615 0x5562fe3a34cc 0x5562fe48347a 0x5562fe3a9f0c 0x7f239fbd85d4 0x7f239fbe0cd4 0x7f239fbb57e0 0x7f238de79665 0x7f238de74f9e 0x7f238de7a13a 0x7f239fbb5d2e 0x7f239f64bb88 0x5562fe3a7098 0x5562fe41a4d9 0x5562fe414ced 0x5562fe3a7bda 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe415c0d 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415c0d 0x5562fe3a7afa\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x5564500fa000 @  0x7f23cad18615 0x5562fe3a34cc 0x5562fe48347a 0x5562fe3a9f0c 0x7f239fbd85d4 0x7f239fbe0cd4 0x7f239fbb57e0 0x7f238de79665 0x7f238de74f9e 0x7f238de7a13a 0x7f239fbb5d2e 0x7f239f64bb88 0x5562fe3a7098 0x5562fe41a4d9 0x5562fe414ced 0x5562fe3a7bda 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe415c0d 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415c0d 0x5562fe3a7afa\n",
            "Epoch 1:   0% 0/124 [00:00<?, ?it/s, loss=28.1, v_num=0]INFO:root:contexts : 말하고 후회할까봐 걱정돼\n",
            "INFO:root:contexts : 짝남이 삐딱하게 굴면 어떻게 하죠?\n",
            "INFO:root:toked ctx: ['<usr>', '▁짝', '남이', '▁', '삐', '딱', '하게', '▁굴', '면', '▁어떻게', '▁하', '죠', '?', '<unused1>', '▁2']\n",
            "INFO:root:toked ctx: ['<usr>', '▁말하고', '▁후회', '할', '까', '봐', '▁걱정', '돼', '<unused1>', '▁0']\n",
            "INFO:root:response : 말은 주워담을 수 없어요.\n",
            "INFO:root:response : 그런 사람에 대한 마음은 정리하는게 좋아요.\n",
            "INFO:root:toked response : ['<sys>', '▁말은', '▁주워', '담을', '▁수', '▁없어', '요.', '</s>']\n",
            "INFO:root:toked response : ['<sys>', '▁그런', '▁사람에', '▁대한', '▁마음은', '▁정리하는', '게', '▁좋아', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁말은', '▁주워', '담을', '▁수', '▁없어', '요.', '</s>']\n",
            "INFO:root:labels ['<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '<unused0>', '▁그런', '▁사람에', '▁대한', '▁마음은', '▁정리하는', '게', '▁좋아', '요.', '</s>']\n",
            "Epoch 1: 100% 124/124 [01:08<00:00,  1.82it/s, loss=27.4, v_num=0]Epoch 1, global step 247: train_loss reached 25.64205 (best 25.64205), saving model to \"/content/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=25.64.ckpt\" as top 1\n",
            "tcmalloc: large alloc 1845223424 bytes == 0x5564500fa000 @  0x7f23cad18615 0x5562fe3a34cc 0x5562fe48347a 0x5562fe3a9f0c 0x7f239fbd85d4 0x7f239fbe0cd4 0x7f239fbb57e0 0x7f238de79665 0x7f238de74f9e 0x7f238de7a13a 0x7f239fbb5d2e 0x7f239f64bb88 0x5562fe3a7098 0x5562fe41a4d9 0x5562fe414ced 0x5562fe3a7bda 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415915 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe415c0d 0x5562fe4149ee 0x5562fe3a7bda 0x5562fe419d00 0x5562fe3a7afa 0x5562fe415c0d 0x5562fe3a7afa\n",
            "Epoch 1: 100% 124/124 [01:23<00:00,  1.48it/s, loss=27.4, v_num=0]Saving latest checkpoint...\n",
            "Epoch 1: 100% 124/124 [01:32<00:00,  1.34it/s, loss=27.4, v_num=0]\n",
            "INFO:root:best model path /content/KoGPT2-chatbot/model_chp/model_-epoch=01-train_loss=25.64.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yDcidi6wFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65070c2f-5080-4102-a105-8b90e0456763"
      },
      "source": [
        "# 대화 테스트\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_torch.py --gpus 1 --chat"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=96, benchmark=False, chat=True, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=5e-05, max_epochs=None, max_len=32, max_steps=None, min_epochs=None, min_steps=None, model_params='model_chp/model_-last.ckpt', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, sentiment='0', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tpu_cores=None, track_grad_norm=-1, train=False, truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "user > 안녕안녕\n",
            "Simsimi > 안녕.\n",
            "user > 뭐해 심심이\n",
            "Simsimi > 저랑 이야기 해요.\n",
            "user > 오 알아듣네 ㅋㅋㅋ\n",
            "Simsimi > 저도 듣고 싶어요.\n",
            "user > 뭐라카노\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 못알아듣누\n",
            "Simsimi > 저도 듣고 싶네요.\n",
            "user > 뭐하누우우\n",
            "Simsimi > 저도 궁금해요.\n",
            "user > 뭐해?\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 뭐하고 놀까?\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 그래 뭐 하고 놀까\n",
            "Simsimi > 저랑 놀아요.\n",
            "user > 가위바위보해\n",
            "Simsimi > 잘 찾아보세요.\n",
            "user > 가위\n",
            "Simsimi > 저도 위로해 드리고 싶네요.\n",
            "user > 바위\n",
            "Simsimi > 조심하세요.\n",
            "user > 보\n",
            "Simsimi > 저도 해보고 싶네요.\n",
            "user > 어떡하지?\n",
            "Simsimi > 저도 궁금해요.\n",
            "user > Traceback (most recent call last):\n",
            "  File \"train_torch.py\", line 199, in chat\n",
            "    q = input('user > ').strip()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train_torch.py\", line 242, in <module>\n",
            "    model.chat()\n",
            "  File \"train_torch.py\", line 213, in chat\n",
            "    print(\"Simsimi > {}\".format(a.strip()))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 129, in __exit__\n",
            "    torch.set_grad_enabled(self.prev)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}